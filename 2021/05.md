---
presentation:
  transition: "none"
  enableSpeakerNotes: true
  margin: 0
---

@import "../common/css/font-awesome-4.7.0/css/font-awesome.css"
@import "../common/css/style-color.css"
@import "../common/css/margin.css"

<!-- slide data-notes="" -->
<div class="header"><img class="hust"></div>

<div class="bottom15"></div>

# 图神经网络导论

<hr class="width50">

## 卷积神经网络

<div class="bottom5"></div>

### 计算机科学与技术学院 &nbsp; &nbsp; 张腾

<br>

#### tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 大纲

@import "../dot/outline-cnn.dot"

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 动机

图像数据集 [ImageNet](https://image-net.org/index.php)：

- 共有 14,197,122 训练图片、50,000 验证图片、100,000 测试图片
- 共有 1,000 个类别，通过众包进行标注
- 图片分辨率：256 × 256、224 × 224、299 × 299

<br>

用全连接网络进行训练 ImageNet

- 图片全部裁减到 224 × 224，输入层神经元个数为 50,176
- 共有 1,000 个类别，输出层神经元个数为 1,000
- 假设只有一个隐藏层，神经元个数取个折中 10,000

<br>

总参数量为 (50,176 + 1,000) × 10,000 = 511,760,000

- 训练效率非常低
- 很容易出现过拟合

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 局部连接 权值共享

@import "../dot/dense-vs-cnn.dot"

<div></div>

局部连接：每个神经元只与前一层固定个 (远小于总数) 神经元相连

<br>

权值共享：固定个神经元均采用相同的输入权重系数

<br>

限制神经元的输入权重个数，降低参数规模，降低模型复杂度

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 局部连接 权值共享

<img src="../tikz/convolution1d.svg" class="center width75 top4 bottom4">

<div class="center">

$$
\begin{align*}
    a_1 & = x_1 \times w_1 + x_2 \times w_2 + x_3 \times w_3 \\
    a_2 & = x_2 \times w_1 + x_3 \times w_2 + x_4 \times w_3 \\
    a_3 & = x_3 \times w_1 + x_4 \times w_2 + x_5 \times w_3 \\
    a_4 & = x_4 \times w_1 + x_5 \times w_2 + x_6 \times w_3
\end{align*}
$$

</div>

<div class="bottom4"></div>

### 卷积神经网络：局部连接，权值共享

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="这页是在讲为啥叫卷积" -->

HEADER 一维卷积

$$
\begin{align*}
    (f \otimes g) [n] = \sum_{m = -\infty}^\infty f[m] \cdot g[n-m]
\end{align*}
$$

<img src="../tikz/convolution1d.svg" class="center width75 top3 bottom4">

取$f[i] = x_i$，$g[-2] = w_3$，$g[-1] = w_2$，$g[0] = w_1$，其余为零，则有

$$
\begin{align*}
    a_n = x_n w_1 + x_{n+1} w_2 + x_{n+2} w_3 = \sum_{m = -\infty}^\infty f[m] \cdot g[n-m] = (f \otimes g) [n]
\end{align*}
$$

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 二维卷积

针对输入是矩阵的情形

<img src="../tikz/convolution2d.svg" class="center width75 top3 bottom4">

参与卷积的深色区域称为对应输出神经元的<span class="blue">感受野</span> (receptive field)

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 二维卷积 图像滤波

平滑去噪

<div class="multi_column top6 left6" style="height:280px">
    <img src="../common/img/tj.jpg" class="height100" >
    <div style="display:flex;align-items:center;height:100%">
        <p class="left2">
            $\otimes ~ \begin{bmatrix}
                \frac{1}{9} & \frac{1}{9} & \frac{1}{9} \\ \frac{1}{9} & \frac{1}{9} & \frac{1}{9} \\ \frac{1}{9} & \frac{1}{9} & \frac{1}{9}
            \end{bmatrix} ~ =$ 
        </p>
    </div>
    <img src="../common/img/tj1.jpg" class="left-2 height100">
</div>

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 二维卷积 图像滤波

边缘提取

<div class="multi_column top6 left6" style="height:280px">
    <img src="../common/img/tj.jpg" class="height100" >
    <div style="display:flex;align-items:center;height:100%">
        <p class="left2">
            $\otimes ~ \begin{bmatrix}
                0 & 1 & 1 \\ -1 & 0 & 1 \\ -1 & -1 & 0
            \end{bmatrix} ~ = $ 
        </p>
    </div>
    <img src="../common/img/tj3.jpg" class="left-2 height100">
</div>

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="这页的卷积模板又叫拉普拉斯算子" -->

HEADER 二维卷积 图像滤波

边缘提取

<div class="multi_column top6 left6" style="height:280px">
    <img src="../common/img/tj.jpg" class="height100" >
    <div style="display:flex;align-items:center;height:100%">
        <p class="left2">
            $\otimes ~ \begin{bmatrix}
                0 & 1 & 0 \\ 1 & -4 & 1 \\ 0 & 1 & 0
            \end{bmatrix} ~ = $
        </p>
    </div>
    <img src="../common/img/tj2.jpg" class="left-2 height100">
</div>

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 拉普拉斯算子

偏导数

$$
\begin{align*}
    \frac{\partial f}{\partial x} = \lim_{u \rightarrow 0} \frac{f(x+u, y) - f(x-u, y)}{2 u} \overset{u \leftarrow 0.5}{\approx} f(x+\frac{1}{2}, y) - f(x-\frac{1}{2}, y)
\end{align*}
$$

于是

$$
\begin{align*}
    \Delta & f = \div (\grad f) = \left[ \frac{\partial}{\partial x}, \frac{\partial}{\partial y} \right] \cdot \left[ \frac{\partial f}{\partial x}; \frac{\partial f}{\partial y} \right] = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} \\
    & = \frac{\partial f(x+\frac{1}{2}, y)}{\partial x} - \frac{\partial f(x-\frac{1}{2}, y)}{\partial x} + \frac{\partial f(x, y+\frac{1}{2})}{\partial y} - \frac{\partial f(x, y-\frac{1}{2})}{\partial y} \\
    & \approx (f(x+1, y) - f(x, y)) - (f(x, y) - f(x-1, y)) \\
    & \qquad \qquad \qquad + (f(x, y+1) - f(x, y)) - (f(x, y) - f(x, y-1)) \\
    & = f(x+1, y) + f(x-1, y) + f(x, y-1) + f(x, y+1) - 4 f(x, y)
\end{align*}
$$

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 卷积超参数

引入参数可以更灵活地进行特征抽取

- 大小$c$：滤波器的长度
- 步长$s$：滤波器滑动的间隔
- 补零$p$：输入神经元两端各补零的个数

<br>

步长$s=2$：

<img src="../tikz/convolution1d-stepsize.svg" class="center width75 top3 bottom4">

若输入神经元个数为$n$，则卷积层神经元个数为$(n - c) / s + 1$

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 卷积超参数

引入参数可以更灵活地进行特征抽取

- 大小$c$：滤波器的长度
- 步长$s$：滤波器滑动的间隔
- 补零$p$：输入神经元两端各补零的个数

<br>

补零$p=1$：

<img src="../tikz/convolution1d-padding.svg" class="center width75 top3 bottom4">

若输入神经元个数为$n$，则卷积层神经元个数为$(n - c + 2p) / s + 1$

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 卷积超参数

引入参数可以更灵活地进行特征抽取

- 大小$c$：滤波器的长度
- 步长$s$：滤波器滑动的间隔
- 补零$p$：输入神经元两端各补零的个数

<br>

若输入神经元个数为$n$，则卷积层神经元个数为$(n - c + 2p) / s + 1$

<br>

常用的卷积有如下三类：

- 窄卷积：步长$s = 1$，两端不补零$p = 0$，卷积后输出长度为$n − c + 1$
- 宽卷积：步长$s = 1$，两端补零$p = c - 1$，卷积后输出长度为$n + c - 1$
- 等宽卷积：步长$s = 1$，两端补零$p = (c − 1) / 2$，卷积后输出长度$n$

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="宽卷积亦可" -->

HEADER 微步卷积

(窄) 卷积可视为高维特征到低维的降维变换，如果想升维怎么办？

<br>

令卷积核的步长 s < 1 也可以实现升维

<br>

实现：在输入特征之间插入 0 间接使得步长变小

<br>

一维微步卷积的例子：

<img src="../tikz/convolution1d-fs.svg" class="width70 left10 top4 bottom4">

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 空洞卷积

对于一个卷积层，如果想增加输出单元的感受野

- 增加卷积核的大小，增加参数数量
- 增加层数，增加参数数量
- 在卷积之前进行汇聚操作，丢失信息

<br>

给卷积核插入“空洞”变相地增加其大小，也称为膨胀卷积

<img src="../tikz/convolution2d-atrous.svg" class="center width60 top3 bottom3">

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 参数求导

设$\Zv = \Av \otimes \Wv + b$，$\Av \in \Rbb^{U \times V}$，$\Wv \in \Rbb^{u \times v}$，$\Zv \in \Rbb^{(U-u+1) \times (V-v+1)}$

$$
\begin{align*}
    z_{ij} = \sum_{u,v} w_{uv} a_{i+u-1, j+v-1} + b
\end{align*}
$$

记$\deltav^\top = \partial \Lcal(\yv, \hat{\yv}) / \partial \Zv$为误差项，由链式法则有

$$
\begin{align*}
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial b} & = \sum_{i,j} \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial z_{ij}} \frac{\partial z_{ij}}{\partial b} = \sum_{i,j} [\deltav]_{ij} \\
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial w_{uv}} & = \sum_{i,j} \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial z_{ij}} \frac{\partial z_{ij}}{\partial w_{uv}} = \sum_{i,j} a_{i+u-1, j+v-1} [\deltav]_{ij} = [\Av \otimes \deltav]_{uv} \\
    & \Longrightarrow \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \Wv} = (\Av \otimes \deltav)^\top
\end{align*}
$$

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 参数求导

设$\Zv = \Av \otimes \Wv + b$，$\Av \in \Rbb^{U \times V}$，$\Wv \in \Rbb^{u \times v}$，$\Zv \in \Rbb^{(U-u+1) \times (V-v+1)}$

$$
\begin{align*}
    z_{ij} = \sum_{u,v} w_{uv} a_{i+u-1, j+v-1} + b
\end{align*}
$$

记$\deltav^\top = \partial \Lcal(\yv, \hat{\yv}) / \partial \Zv$为误差项，由链式法则有

$$
\begin{align*}
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial a_{st}} = \sum_{i,j} \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial z_{ij}} \frac{\partial z_{ij}}{\partial a_{st}} = \sum_{i,j} w_{s-i+1,t-j+1} [\deltav]_{ij}
\end{align*}
$$

当$s-i+1 \not \in [u]$或者$t-j+1 \not \in [v]$时，令$w_{s-i+1,t-j+1} = 0$

相当于对$\Wv$进行了$p = (U - u, V - v)$的零填充，记$\widetilde{\otimes}$为<span class="blue">宽卷积</span>

$$
\begin{align*}
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \Av} = (\rot(\Wv) \widetilde{\otimes} \deltav)^\top
\end{align*}
$$

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 汇聚

汇聚 (pooling) 层也叫子采样 (subsampling) 层

<br>

- 最大汇聚 (maximum pooling)：取区域内神经元最大值，<span class="blue">拥有一定的平移不变性</span>

<img src="../tikz/pooling-max.svg" class="center width50 top3 bottom3">

- 平均汇聚 (mean pooling)：取区域内神经元平均值

<br>

我的批注 将区域下采样为一个值，进一步减少网络参数，降低模型复杂度

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 卷积神经网络

卷积神经网络由卷积层、汇聚层、全连接层交叉堆叠而成

@import "../dot/cnn.dot"

<br>

趋势

- 更小的卷积核，比如 3 × 3
- 更深的结构，比如层数大于 50
- 汇聚层的作用可由卷积步长代替，使用比例逐渐降低，趋向于全卷积网络

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 反向传播

卷积神经网络也是通过误差反向传播算法进行参数学习

<br>

卷积神经网络的参数为<span class="blue">卷积核</span>和<span class="blue">偏置</span>

<br>

设第$l$层为卷积层：$\Zv_l = \Av_{l-1} \otimes \Wv_l + b_l$，$\partial \Lcal(\yv, \hat{\yv}) / \partial \Zv_l = \deltav_l^\top$为误差项

$$
\begin{align*}
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \Wv_l} & = (\Av_{l-1} \otimes \deltav_l)^\top \\
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial b_l} & = \sum_{i,j} [\deltav_l]_{ij} \\
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \Av_{l-1}} & = (\rot(\Wv_l) \widetilde{\otimes} \deltav_l)^\top
\end{align*}
$$

剩下需计算$\deltav_l$

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 反向传播

若第$l+1$层为汇聚层：该层神经元均是第$l$层某个区域下采样得到的

$$
\begin{align*}
    \Zv_l \xrightarrow{激活} \Av_l \xrightarrow{汇聚} \Zv_{l+1}
\end{align*}
$$

<br>

反向传播时，需对第$l+1$层的误差项<span class="green">上采样</span>到和第$l$层一样大

$$
\begin{align*}
    \deltav_l^\top = \frac{\partial \Lcal(\yv, \hat{\yv})}{\partial \Zv_l} & = \class{red}{\frac{\partial \Lcal(\yv, \hat{\yv})}{\partial \Zv_{l+1}}} \class{green}{\frac{\partial \Zv_{l+1}}{\partial \Av_l}} \class{blue}{\frac{\partial \Av_l}{\partial \Zv_l}} \\
    & = \class{green}{\up}(\class{red}{\deltav_{l+1}^\top}) \odot \class{blue}{h'_l (\Zv_l)}
\end{align*}
$$

- 最大汇聚：误差项$\deltav_{l+1}$中的每个值会直接传递到第$l$层对应区域中的最大值所对应的神经元，该区域中其它神经元的误差项的都设为$0$
- 平均汇聚：误差项$\deltav_{l+1}$中每个值会被平均分配到第$l$层对应区域中的所有神经元

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 反向传播

若第$l+1$层为卷积层：该层神经元均是第$l$层神经元经过卷积得到的

$$
\begin{align*}
    \Zv_l \xrightarrow{激活} \Av_l \xrightarrow{汇聚} \Zv_{l+1}
\end{align*}
$$

<br>

反向传播

$$
\begin{align*}
    \deltav_l^\top = \frac{\partial \Lcal(\yv, \hat{\yv})}{\partial \Zv_l} = \frac{\partial \Lcal(\yv, \hat{\yv})}{\partial \Av_l} \frac{\partial \Av_l}{\partial \Zv_l} = (\rot(\Wv_{l+1}) \widetilde{\otimes} \deltav_{l+1})^\top h'_l (\Zv_l)
\end{align*}
$$

其中$\widetilde{\otimes}$为宽卷积

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="90 年代被美国很多银行用来识别支票上面的手写数字" -->

HEADER 经典网络 LeNet-5

<img src="../tikz/lenet.svg" class="center width75 top1 bottom0">

```python {.line-numbers}
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 28, 28, 6)         156
_________________________________________________________________
average_pooling2d (AveragePo (None, 14, 14, 6)         0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 10, 10, 16)        2416
_________________________________________________________________
average_pooling2d_1 (Average (None, 5, 5, 16)          0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 1, 120)         48120
_________________________________________________________________
flatten (Flatten)            (None, 120)               0
_________________________________________________________________
dense (Dense)                (None, 84)                10164
_________________________________________________________________
dense_1 (Dense)              (None, 10)                850
=================================================================
Total params: 61,706
Trainable params: 61,706
Non-trainable params: 0
```

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 经典网络 LeNet-5

```python {.line-numbers}
import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.datasets.mnist import load_data
from tensorflow.keras.layers import (Conv2D, Dense, Dropout,
                                     Flatten, AveragePooling2D)
from tensorflow.keras.optimizers import Adam

(x_train, y_train), (x_test, y_test) = load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

model = Sequential()
model.add(Conv2D(6, (5, 5), activation="relu",
                 padding="same", input_shape=(28, 28, 1)))
model.add(AveragePooling2D(pool_size=(2, 2)))
model.add(Conv2D(16, (5, 5), activation="relu"))
model.add(AveragePooling2D(pool_size=(2, 2)))
model.add(Conv2D(120, (5, 5), activation="relu"))
model.add(Flatten())
model.add(Dense(84, activation="relu"))
model.add(Dense(10, activation="softmax"))
model.summary()

model.compile(
    optimizer=Adam(0.001),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test, verbose=2)

Epoch 1/5
1875/1875 [=============] - 5s 2ms/step - loss: 0.2169 - accuracy: 0.9316
Epoch 2/5
1875/1875 [=============] - 3s 1ms/step - loss: 0.0733 - accuracy: 0.9773
Epoch 3/5
1875/1875 [=============] - 3s 1ms/step - loss: 0.0525 - accuracy: 0.9833
Epoch 4/5
1875/1875 [=============] - 3s 1ms/step - loss: 0.0417 - accuracy: 0.9869
Epoch 5/5
1875/1875 [=============] - 3s 1ms/step - loss: 0.0330 - accuracy: 0.9898

313/313 - 0s - loss: 0.0322 - accuracy: 0.9905
```

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="第一个现代卷积网络模型" -->

HEADER 经典网络 AlexNet

2012 年 ImageNet 图像分类竞赛冠军

1. 输入层，输入图像大小为 224 × 224 × 3
2. 卷积层，使用两个 11 × 11 × 3 × 48 的卷积核，步长 s = 4，零填充 p = 3
3. 汇聚层，采样窗口为 3 × 3，使用最大汇聚，步长 s = 2
4. 卷积层，共使用两个 5 × 5 × 48 × 128 的卷积核，步长 s = 1，零填充 p = 2
5. 汇聚层，采样窗口为 3 × 3，使用最大汇聚，步长 s = 2
6. 卷积层，使用一个 3 × 3 × 256 × 384 的卷积核，步长 s = 1，零填充 p = 1
7. 卷积层，使用两个 3 × 3 × 192 × 192 的卷积核，步长 s = 1，零填充 p = 1
8. 卷积层，使用两个 3 × 3 × 192 × 128 的卷积核，步长 s = 1，零填充 p = 1
9. 汇聚层，采样窗口为 3 × 3，使用最大汇聚，步长 s = 2
10. 三个全连接层
11. 输出层

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 经典网络 Inception

一个卷积层包含多个不同大小的卷积操作

@import "../dot/inception.dot"

<div></div>

我的批注 1 × 1 卷积就是将所有通道的特征直接相加

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 经典网络 GoogLeNet

{--Google--}， GoogLe ✓，致敬 ‪Yann LeCun

<br>

2014 年 ImageNet 图像分类竞赛的冠军

<br>

由 9 个 Inception v1 模块和 5 个汇聚层以及其它一些卷积层和全连接层构成，总共为 22 层网络

<br>

Inception 网络有多个改进版本

- v3 网络用多层的小卷积核来替换大的卷积核，以减少计算量和参数量
- v4 网络结合了残差网络的设计，带有直连边

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 经典网络复用

使用在 ImageNet 训练好的残差网络 ResNet50 进行图像分类

```python {.line-numbers}
import numpy as np
from tensorflow.keras.applications import resnet50
from tensorflow.keras.preprocessing import image

model = resnet50.ResNet50(weights='imagenet')
img = image.load_img('../common/img/tj224x224.jpg', target_size=(224,224))

# RGB: (224,224) → (224, 224, 3) 灰度图: (224,224) → (224, 224, 1)
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)  # batch_size = 1
x = resnet50.preprocess_input(x)  # 中心化
preds = model.predict(x)
resnet50.decode_predictions(preds, top=5)[0]

('n03630383', 'lab_coat', 0.24623604),     实验服
('n03877472', 'pajama', 0.17045474),       睡衣
('n04317175', 'stethoscope', 0.095500074), 听诊器
('n04479046', 'trench_coat', 0.07988542),  军用雨衣
('n03617480', 'kimono', 0.055965725),      和服
```

<img src="../common/img/tj.jpg" style="height:250px;width:250px;margin-left:auto;margin-right:2.5rem;margin-top:-26%">

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="从这个例子也可以看出全连接层的参数量巨大，fc1超过1亿参数，总网络不到1亿4千万" -->

HEADER 经典网络复用

使用 VGG16 提取特征

```python {.line-numbers}
import numpy as np
from tensorflow.keras.applications import vgg16
from tensorflow.keras.preprocessing import image

vgg16.VGG16(weights='imagenet').summary()
model = vgg16.VGG16(weights='imagenet', include_top=False)

img = image.load_img('../common/img/tj224x224.jpg', target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)  # batch_size = 1
x = vgg16.preprocess_input(x)  # 中心化

feat = model.predict(x)
feat.shape

Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 224, 224, 3)]     0
_________________________________________________________________
......
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
predictions (Dense)          (None, 1000)              4097000
=================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
_________________________________________________________________
(1, 7, 7, 512)
```

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 经典网络复用

从 VGG19 的任意中间层中抽取特征

```python {.line-numbers}
import numpy as np
from tensorflow.keras.applications import vgg19
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image

base_model = vgg19.VGG19(weights='imagenet')
base_model.summary()

model = Model(inputs=base_model.input,
              outputs=base_model.get_layer('block4_pool').output)

img = image.load_img('../common/img/tj224x224.jpg', target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)  # batch_size = 1
x = vgg19.preprocess_input(x)  # 中心化

feat = model.predict(x)
print(feat.shape)

Model: "vgg19"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 224, 224, 3)]     0
_________________________________________________________________
......
_________________________________________________________________
block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
predictions (Dense)          (None, 1000)              4097000
=================================================================
Total params: 143,667,240
Trainable params: 143,667,240
Non-trainable params: 0
_________________________________________________________________
(1, 14, 14, 512)
```

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 经典网络复用

在新数据上微调 InceptionV3

```python {.line-numbers}
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

base_model = InceptionV3(weights='imagenet', include_top=False)
for i, layer in enumerate(base_model.layers):
    print(i, layer.name)

x = base_model.output
x = GlobalAveragePooling2D()(x)  # 全局平均池化层
x = Dense(1024, activation='relu')(x)  # 全连接层
predictions = Dense(200, activation='softmax')(x)  # 输出层 假设有200个类
model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False  # 锁住所有InceptionV3的层

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
model.fit(...)  # 在新的数据集上训练新加层的参数

for layer in model.layers[:249]:
    layer.trainable = False  # 锁住当前网络的前250层
for layer in model.layers[249:]:
    layer.trainable = True

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
model.fit(...)

0 input_1
1 conv2d
2 batch_normalization
3 activation
4 conv2d_1
5 batch_normalization_1
6 activation_1
......
305 batch_normalization_93
306 activation_85
307 mixed9_1
308 concatenate_1
309 activation_93
310 mixed10
```

FOOTER3 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn
