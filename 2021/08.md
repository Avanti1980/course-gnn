---
presentation:
  transition: "none"
  enableSpeakerNotes: true
  margin: 0
---

@import "../common/css/font-awesome-4.7.0/css/font-awesome.css"
@import "../common/css/style-color.css"
@import "../common/css/margin.css"

<!-- slide data-notes="" -->
<div class="header"><img class="hust"></div>

<div class="bottom15"></div>

# 图神经网络导论

<hr class="width50">

## 图神经网络

<div class="bottom5"></div>

### 计算机科学与技术学院 &nbsp; &nbsp; 张腾

<br>

#### tengzhang@hust.edu.cn

<!-- slide data-notes="这里要说一下编码器就是在做信息提取 不一定是RNN 也可以是CNN 比如为图片生成标题或描述的任务" -->

HEADER 编码器 - 解码器模型

<img src="../tikz/seq2seq-asyn.svg" class="width80 center top5 bottom5">

编码器：表示学习

- 网格型数据，可采用卷积神经网络
- 序列型数据，可采用循环神经网络

<br>

对于一般的图数据，编码器该如何设计？

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 编码器设计

输入：$\Gcal = (\Vcal, \Ecal)$，$\Xv = [\xv_1, \ldots, \xv_{|\Vcal|}] \in \Rbb^{d \times |\Vcal|}$，边的特征，整图的特征

<br>

一个 naïve 的做法是将邻接矩阵$\Av$拉直成向量，后接一个全连接网络

$$
\begin{align*}
    \zv_\Gcal = \mlp ([\Av]_{:,1} \oplus [\Av]_{:,2} \oplus \cdots \oplus [\Av]_{:,|\Vcal|})
\end{align*}
$$

邻接矩阵人为给点指定了顺序，点和边自带的特征没有用起来

引入置换矩阵$\Pv$，定义

- 平移不变性：$f(\Pv \Av \Pv^\top) = f(\Av)$，<span class="blue">标量值函数</span>对于点的标号顺序不敏感
- 平移等价性：$\fv (\Pv \Av \Pv^\top) = \Pv \fv (\Av)$，<span class="blue">向量值函数</span>与点的标号顺序始终一致

<br>

神经消息传递 (neural message passing) 框架：GNN 的统一表示

- 点之间不断进行信息交互
- 信息通过神经网络进行更新

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 消息传递框架

记第$k$轮点$u$的隐藏状态 (嵌入表示) 为$\hv_u^\sup{k}$，GNN 的消息传递形式化为

$$
\begin{align*}
    \hv_u^\sup{0}   & = \xv_u, ~ \forall u \in \Vcal                                                                                         \\
    \hv_u^\sup{k+1} & = \update^\sup{k} \left( \hv_u^\sup{k}, \aggregate^\sup{k} ( \{ \hv_v^\sup{k} \mid \forall v \in \Ncal(u) \} ) \right) \\
                    & = \update^\sup{k} \left( \hv_u^\sup{k}, \mv_{\Ncal(u)}^\sup{k} \right)                                                 \\
    \zv_u           & = \hv_u^\sup{K}, ~ \forall u \in \Vcal
\end{align*}
$$

- $\update (\cdot)$：用来更新当前点隐藏状态的可微函数
- $\aggregate (\cdot)$：用来汇聚邻居信息的可微函数，其输入是一个集合 (平移等价性)
- $\mv_{\Ncal(u)}^\sup{k}$：点$u$的所有邻居的信息汇聚结果
- 如果点本身没有特征，可以用点层面的统计量作为特征
- GNN 一层可视为一轮消息传递，$K$层即编码了所有$K$阶邻居的信息

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 消息传递框架

<img src="../tikz/gnn.svg" class="center width80 top10">

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER GNN 的基本形式

最早版本的 GNN：

$$
\begin{align*}
    \hv_u^\sup{k} = \sigma \left( \Wv_\self^\sup{k} ~ \hv_u^\sup{k-1} + \Wv_\neigh^\sup{k} \sum_{v \in \Ncal(u)} \hv_v^\sup{k-1} + \bv^\sup{k} \right)
\end{align*}
$$

- $\Wv_\self^\sup{k}$、$\Wv_\neigh^\sup{k}$、$\bv^\sup{k}$是可训练的参数，偏置$\bv^\sup{k}$通常可以省略
- $\sigma$是逐元素的非线性变换，例如之前介绍过的各种激活函数

<br>

用消息传递框架表示：

- $\aggregate ( \{ \hv_v \mid \forall v \in \Ncal(u) \} ) = \mv_{\Ncal(u)} = \class{blue}{\sum_{v \in \Ncal(u)} \hv_v}$，注意求和是点序无关的
- $\update ( \hv_u, \mv_{\Ncal(u)} ) = \sigma ( \Wv_\self ~ \hv_u + \Wv_\neigh ~ \mv_{\Ncal(u)} + \bv)$

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER GNN 的基本形式

记$\Hv^\sup{k} = [\hv_1^\sup{k}, \ldots, \hv_{|\Vcal|}^\sup{k}]$

GNN 的基本形式的矩阵表示：

$$
\begin{align*}
    \Hv^\sup{k} = \sigma ( \Wv_\self^\sup{k} ~ \Hv^\sup{k-1} + \Wv_\neigh^\sup{k} \Hv^\sup{k-1} \Av )
\end{align*}
$$

<br>
    
添加自指向环，将显式的$\update(\cdot)$隐藏于$\aggregate(\cdot)$之中

$$
\begin{align*}
    \hv_u^\sup{k} & = \aggregate^\sup{k} ( \{ \hv_v^\sup{k} \mid \forall v \in \Ncal(u) \class{blue}{\cup \{u\}} \} ) \\
    \Hv^\sup{k} & = \sigma (\Wv^\sup{k} \Hv^\sup{k-1} (\Av \class{blue}{+ \Iv}))
\end{align*}
$$

- 优点：共享$\Wv_\self$和$\Wv_\neigh$，可以避免过拟合
- 缺点：$\aggregate(\cdot)$要求平移等价性，这样无法区分自身的信息和邻居的信息

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER GNN 的各种魔改

@import "../dot/gnn.dot"

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="直接加和对度数敏感 大v说了算" -->

HEADER 归一化

GNN 基本形式采用直接加和汇聚邻居的信息

<br>

问题：对度数敏感，若$d_u \gg d_v$，则$\| \sum_{w \in \Ncal(u)} \hv_w \| \gg \| \sum_{w \in \Ncal(v)} \hv_w \|$，$u$的信息可以“淹没”其它点的信息

<br>

方案：归一化

$$
\begin{align*}
    \mv_{\Ncal(u)} = \frac{\sum_{v \in \Ncal(u)} \hv_v}{|\Ncal(u)|}, \quad \mv_{\Ncal(u)} = \sum_{v \in \Ncal(u)} \frac{\hv_v}{\sqrt{|\Ncal(u)||\Ncal(v)|}}
\end{align*}
$$

后者与自指向环结合：图卷积网络 (**g**raph **c**onvolutional **n**etwork, GCN)

$$
\begin{align*}
    \hv_u^\sup{k} = \sigma \left( \Wv^\sup{k} \sum_{ v \in \Ncal(u) \cup \{u\} } \frac{\hv_v}{\sqrt{|\Ncal(u)||\Ncal(v)|}} \right)
\end{align*}
$$

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="前面的加权求和还是太简单了" -->

HEADER 集合汇聚

归一化 → 具有置换不变性的集合函数

<br>

集合汇聚 (set pooling)

$$
\begin{align*}
    \mv_{\Ncal(u)} = \mlp_\thetav \left( \sum_{v \in \Ncal(u)} \mlp_\phiv (\hv_v) \right)
\end{align*}
$$

理论上可以证明任意具有置换不变性的集合函数都可由上式无限逼近

<br>

Janossy 汇聚：通过考虑所有的标号排列获得置换不变性

$$
\begin{align*}
    \mv_{\Ncal(u)} = \mlp_\thetav \left( \frac{1}{|\Pi|} \sum_{\pi \in \Pi} \rho_\phi (\hv_{v_1}, \hv_{v_2}, \ldots, \hv_{v_{|\Ncal(u)|}})_{\pi_i} \right)
\end{align*}
$$

其中$\Pi$是由置换构成的集合，$\rho_\phi$是任意函数 (不需要具有置换不变性)

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 邻居注意力

为邻居赋予不同的权重：$\mv_{\Ncal(u)} = \sum_{v \in \Ncal(u)} \alpha_{u,v} \hv_v$

<br>

图注意力网络 (**g**raph **at**tention network, GAT)

$$
\begin{align*}
    \alpha_{u,v} = \frac{\exp(\av^\top [\Wv \hv_u \oplus \Wv \hv_v])}{\sum_{v' \in \Ncal(u)} \exp(\av^\top [\Wv \hv_u \oplus \Wv \hv_{v'}])}
\end{align*}
$$

其中$\av$和$\Wv$是可训练的参数，$\oplus$表示向量拼接

<br>

变种：

$$
\begin{align*}
    \alpha_{u,v} = \frac{\exp(\av^\top [\hv_u^\top \Wv \hv_v])}{\sum_{v' \in \Ncal(u)} \exp(\av^\top [\hv_u^\top \Wv \hv_{v'}])}, ~ \alpha_{u,v} = \frac{\exp(\mlp(\hv_u, \hv_v))}{\sum_{v' \in \Ncal(u)} \exp(\mlp(\hv_u, \hv_{v'}))}
\end{align*}
$$

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 更新

对更新进行魔改主要为了解决 GNN 的过平滑 (over-smoothing) 问题

<br>

过平滑：当 GNN 的层数 (迭代轮数) 过大时，所有点的信息趋于相同

<br>

设$f: \Rbb_+ \mapsto \Rbb_+$为可微的归一化函数，对任意带有自指向环并采用

$$
\begin{align*}
    \aggregate ( \{ \hv_v, \forall v \in \Ncal(u) \cup \{u\} \} ) = \frac{1}{f (|\Ncal(u) \cup \{u\}|)} \sum_{v \in \Ncal(u) \cup \{u\}} \hv_v
\end{align*}
$$

进行信息汇聚的 GNN 有$\ev^\top (\partial \hv_v^\sup{k} / \partial \hv_u^\sup{0}) \ev \propto p_K (u | v)$

<br>

当$K$过大时，$p_K (u | v)$即为随机游走的平稳分布，这意味着每个点的局部邻居信息已经完全丢失

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 跳连

基本想法：类似于残差网络，引入跳连 (skip connection) 凸显点自身的信息，防止被邻居信息“淹没”

<br>

拼接跳连：初见于 GraphSAGE 算法

$$
\begin{align*}
    \update_{\text{concat}} (\hv_u, \mv_{\Ncal(u)}) = [\update_{\text{base}} (\hv_u, \mv_{\Ncal(u)}) \oplus \hv_u]
\end{align*}
$$

插值跳连：

$$
\begin{align*}
    \update_{\text{interpolate}} (\hv_u, \mv_{\Ncal(u)}) = \alphav_1 \odot \update_{\text{base}} (\hv_u, \mv_{\Ncal(u)}) + \alphav_2 \odot \hv_u
\end{align*}
$$

其中$\alphav_1, \alphav_2 \in [0,1]^d$为门向量 (gating vectors)，$\alphav_2 = \ev - \alphav_1$，$\alphav_1$与 GNN 原本的参数联合学习

- $\alphav_1$可以作为一个单层 GNN 的输出
- $\alphav_1$也可以作为一个 MLP 的输出

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="这里说一下动态图可以用RNN来做 每个点上跑一个RNN" -->

HEADER 门机制

对于点$u$：

- 每轮汇聚得到的邻居信息$\mv_{\Ncal(u)}^\sup{k}$构成一个序列，可以看作 RNN 的输入$\xv^\sup{k}$
- 每轮更新得到的嵌入表示$\hv_u^\sup{k}$构成一个序列，可以看作 RNN 的隐藏状态

$$
\begin{align*}
    \hv_u^\sup{k} & = \gru (\hv_u^\sup{k-1}, \mv_{\Ncal(u)}^\sup{k}) \\
    \hv_u^\sup{k} & = \lstm (\hv_u^\sup{k-1}, \mv_{\Ncal(u)}^\sup{k})
\end{align*}
$$

事实上任何 RNN 都可引入到 GNN 中来，不局限于 GRU 和 LSTM

<br>

权值共享

- 在每个点上训练一个 RNN，不同点之间权值共享
- 通常丹师们还会将 GNN 的汇聚层也进行权值共享

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 全层拼接

之前我们都是假设将 GNN 的最后一层当作点的嵌入表示$\zv_u = \hv_u^\sup{k}$，但层数一深最后一层各点的嵌入表示趋于相同 (过平滑)

<br>

拼接所有层的嵌入表示，称为 **j**umping **k**nowledge (JK) connections

$$
\begin{align*}
    \zv_u = f_{\text{JK}} (\hv_u^\sup{0} \oplus \hv_u^\sup{1} \oplus \cdots \oplus \hv_u^\sup{K})
\end{align*}
$$

其中$f_{\text{JK}}(\cdot)$是任意可微函数：

- 恒等函数
- max-pooling
- LSTM

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="边分类型了" -->

HEADER 异构图

关系图卷积网络 (**r**elational **g**raph **c**onvolutional **n**etwork, RGCN)

$$
\begin{align*}
    \mv_{\Ncal(u)} = \sum_{\tau \in \Rcal} \sum_{v \in \Ncal_\tau (u)} \frac{\Wv_\tau \hv_v}{f_n(\Ncal(u), \Ncal(v))}
\end{align*}
$$

其中$f_n(\cdot, \cdot)$是归一化函数，之前介绍过的归一化函数都可用过来

<br>

权值共享：引入一组基矩阵$\Bv_1, \ldots, \Bv_b$

$$
\begin{align*}
    \mv_{\Ncal(u)} = \sum_{\tau \in \Rcal} \sum_{v \in \Ncal_\tau (u)} \frac{\sum_{i \in [b]} \alpha_{i, \tau} \Bv_i \hv_v }{f_n(\Ncal(u), \Ncal(v))}
\end{align*}
$$

扩展

- 不带权值共享
- 加入注意力机制

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 加权图

RGCN 假设边的的特征是离散的，因此汇聚可以采用直接求和的形式

<br>

对于更一般形式的边的特征，可以和邻居的嵌入表示进行拼接

$$
\begin{align*}
    \mv_{\Ncal(u)} = \aggregate_\base ( \{ \hv_v \oplus \ev_{(u, \tau, v)}, \forall v \in \Ncal(u) \} )
\end{align*}
$$

其中$\ev_{(u, \tau, v)}$是边$(u, \tau, v)$的向量值特征

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 图汇聚

对于图层面的任务，还需整图的嵌入表示$\zv_\Gcal$

<br>

集合汇聚策略：学习汇聚函数$f_{\mathrm{p}}: \{ \zv_1, \ldots, \zv_{|\Vcal|} \} \mapsto \zv_\Gcal$

$$
\begin{align*}
    \zv_\Gcal = \frac{\sum_{u \in \Vcal} \zv_u}{f_n(|\Vcal|)}
\end{align*}
$$

<br>

事实上之前介绍的任意对邻居的汇聚函数都可用过来

套用 LSTM 和注意力机制，初始化$\qv_0$、$\ov_0$为全零向量，对$t \in [T]$有

$$
\begin{align*}
    \qv_t = \lstm(\ov_{t−1}, \qv_{t−1}), ~ \alpha_{v,t} & = \frac{\exp(f_a (\zv_t, \qv_t))}{\sum_{u \in \Vcal} \exp(f_a (\zv_u, \qv_t))}, ~ \ov_t = \sum_{v \in \Vcal} \alpha_{v,t} \zv_v
\end{align*}
$$

最终输出$\zv_\Gcal = \ov_1 \oplus \ov_2 \oplus \cdots \oplus \ov_T$

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 图汇聚

集合汇聚没有考虑图的结构信息

<br>

图粗化 (coarsening) 策略：构建层次化的 GNN，类似于 CNN 中的汇聚

- 假设有聚类函数$\fv_c: \Gcal \times \Rbb^{|\Vcal| \times d} \mapsto \Rbb^{|\Vcal| \times c}$将图中的点分成$c$个簇
- 记$\Sv = \fv_c(\Gcal, \Zv)$为分配矩阵，$[\Sv]_{u,i}$表示点$u$和第$i$个簇之间的相关性
- $f_c$简单的可采用谱聚类，复杂的也可再用一个 GNN

<br>

以簇为点构造新的粗粒度的图，对应的邻接矩阵和点特征矩阵：

$$
\begin{align*}
    \Av_\new = \Sv^\top \Av \Sv \in \Rbb^{c \times c}, \quad \Xv_\new = \Sv^\top \Xv \in \Rbb^{c \times d}
\end{align*}
$$

- 如果新图粒度已然合适，就在新图上再运行一个 GNN 得到各点 (原图中的簇) 的嵌入表示，然后用集合汇聚得到整图的嵌入表示；否则继续聚类
- 如果想整个模型是端到端的，那么聚类函数$f_c$必须是可微的
- 如果想改善模型的计算复杂度和训练速度，可采用删点而不是汇聚

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 一般形式的消息传递

之前介绍的汇聚-更新基本模式可以进一步一般化

<br>

引入边和图的嵌入表示

$$
\begin{align*}
    \hv_{(u,v)}^\sup{k} & = \class{blue}{\update_\edge} (\hv_{(u,v)}^\sup{k-1}, \hv_u^\sup{k-1}, \hv_v^\sup{k-1}, \hv_\Gcal^\sup{k-1}) \\
    \mv_{\Ncal(u)} & = \aggregate_\node (\{ \hv_{(u,v)}^\sup{k}, \forall v \in \Ncal(u) \}) \\
    \hv_u^\sup{k} & = \update_\node (\hv_u^\sup{k-1}, \mv_{\Ncal(u)}, \class{blue}{\hv_\Gcal^\sup{k-1}} ) \\
    \hv_\Gcal^\sup{k} & = \class{blue}{\update_\graph} (\hv_\Gcal^\sup{k-1}, \{ \hv_u^\sup{k}, \forall u \in \Vcal \}, \{ \hv_{(u,v)}^\sup{k}, \forall (u,v) \in \Ecal \})
\end{align*}
$$

- 每轮的更新顺序是先边、再点，最后整图
- 点更新引入了整图的嵌入表示，即同时考虑局部和全局信息
- 整图更新可以采用前面介绍的汇聚、粗化等策略
- 近期有研究表明一般形式的消息传递更适合学习逻辑表达式

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER GNN 实现 点分类

设编码器给出的点的嵌入表示为$\zv_u$，类别标记$\yv_u \in \{ 0,1 \}^C$

<br>

解码器的损失函数通常采用负对数似然

$$
\begin{align*}
    \Lcal = - \sum_{u \in \Vcal_\train} \log \left( \sum_{c \in [C]} [\yv_u]_c \frac{\exp(\wv_c^\top \zv_u)}{\sum_{i \in [C]} \exp(\wv_i^\top \zv_u)} \right)
\end{align*}
$$

其中$\wv_1, \ldots, \wv_C$是待学习的参数

<br>

两类学习任务

- 转导学习：待预测的点在训练时已知，参与消息传递，不参与损失函数计算
- 归纳学习：待预测的点及相关的边在训练时未知，既不参与消息传递，也不参与损失函数计算，通常用于有多个连通分支的图，在一个连通分支上训练，在另一个连通分支上预测

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 图层面的任务

设图的嵌入表示为$\zv_\Gcal$

<br>

分类，训练数据为$(\Gcal_1, \yv_1), \ldots, (\Gcal_m, \yv_m)$，类别标记$\yv_j \in \{ 0,1 \}^C$，损失采用负对数似然

$$
\begin{align*}
    \Lcal = - \sum_{j \in [m]} \log \left( \sum_{c \in [C]} [\yv_j]_c \frac{\exp(\wv_c^\top \zv_{\Gcal_j})}{\sum_{i \in [C]} \exp(\wv_i^\top \zv_{\Gcal_j})} \right)
\end{align*}
$$

其中$\wv_1, \ldots, \wv_C$是待学习的参数

<br>

回归，训练数据为$(\Gcal_1, y_1), \ldots, (\Gcal_m, y_m)$，损失采用平方损失函数

$$
\begin{align*}
    \Lcal = - \sum_{i \in [m]} \| \mlp(\zv_{\Gcal_i}) - y_{\Gcal_i} \|_2^2
\end{align*}
$$

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="这里求和对所有D中的边求和就是在做边预测" -->

HEADER 关系预测

编码器给出的点的嵌入表示为$\zv_u$

<br>

关系预测的问题形式化：

$$
\begin{align*}
    \Lcal = \sum_{(u,v) \in \Dcal} l(\dec(\zv_u, \zv_v), [\Sv]_{uv})
\end{align*}
$$

其中

- $\Dcal$是训练点对集合
- $l(\cdot, \cdot)$是损失函数
- $\dec(\cdot, \cdot)$是解码器
- $[\Sv]_{uv}$描述点$u$、$v$间的关系

后三者在选择上可以排列组合

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="这里可以讲一下拉普拉斯矩阵" -->

HEADER 基于矩阵分解的方法

拉普拉斯特征映射：

- $\dec(\zv_u, \zv_v) = \| \zv_u - \zv_v \|_2^2$
- $\Lcal = \sum_{(u,v) \in \Dcal} \dec (\zv_u, \zv_v) ~ [\Sv]_{uv}$

即相似的点要有相似的嵌入表示，若$\Sv$为拉普拉斯矩阵，$\zv_u \in \Rbb^d$，则最优解就是$\Sv$的最小$d$个特征向量，事实上就是在做谱聚类

<br>

内积法：

- $\dec(\zv_u, \zv_v) = \zv_u^\top \zv_v$
- $\Lcal = \sum_{(u,v) \in \Dcal} \| \dec (\zv_u, \zv_v) - [\Sv]_{uv} \|_2^2$

代表性方法有 Graph Factorization (GF)、GraRep、HOPE，区别仅在于采用的$\Sv$不同，GF 直接用$\Sv = \Av$，GraRep 用$\Av$的幂次，HOPE 可用之前介绍的任意两点间的局部重合度量

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="这里的方法来自于nlp中的word2vec，拟合共现概率" -->

HEADER 基于随机游走的方法

基于矩阵分解的方法均采用固定的$\Sv$，多为邻接矩阵$\Av$的多项式函数

采用随机、非对称的$\Sv$：

$$
\begin{align*}
    \dec(\zv_u, \zv_v) & = \frac{\exp((\Wv_{\cen} \zv_u)^\top (\Wv_{\con} \zv_v))}{\sum_{k \in \Vcal} \exp((\Wv_{\cen} \zv_u)^\top (\Wv_{\con} \zv_k))} \approx P_T (v|u) \\
    \Lcal & = - \sum_{(u,v) \in \Dcal} \log ( \dec(\zv_u, \zv_v) )
\end{align*}
$$

- $P_T (v|u)$为从点$u$开始进行$T$步随机游走访问到$v$的概率
- $\Dcal$是从$u$开始随机游走序列构成的集合

损失函数涉及 Softmax 操作，计算复杂度$O(|\Dcal||\Vcal|)$

- DeepWalk 采用结构化 Softmax 将计算复杂度降低到$O(|\Dcal|\log|\Vcal|)$
- node2vec 添加了负采样$\Lcal = - \sum_{(u,v) \in \Dcal} \log ( \sigma (\zv_u^\top \zv_v) ) - \gamma \Ebb_{w \sim P(\Vcal)} [\log (-\sigma (\zv_u^\top \zv_w) )]$

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 关系预测

问题形式化：

$$
\begin{align*}
    \Lcal = \sum_{(u,v) \in \Dcal} l(\dec(\zv_u, \zv_v), [\Sv]_{uv})
\end{align*}
$$

方法总结：

<div class="threelines">

|    方法     |                                解码器                                |             相似性度量             |                   损失函数                   |
| :---------: | :------------------------------------------------------------------: | :--------------------------------: | :------------------------------------------: |
|  Lap. eig.  |                       $\| \zv_u - \zv_v \|^2$                        |                随意                |    $\dec(\zv_u, \zv_v) \cdot [\Sv]_{uv}$     |
| Graph Fact. |                          $\zv_u^\top \zv_v$                          |            $[\Av]_{uv}$            | $\| \dec (\zv_u, \zv_v) - [\Sv]_{uv} \|_2^2$ |
|   GraRep    |                          $\zv_u^\top \zv_v$                          | $[\Av]_{uv}, \ldots, [\Av^k]_{uv}$ | $\| \dec (\zv_u, \zv_v) - [\Sv]_{uv} \|_2^2$ |
|    HOPE     |                          $\zv_u^\top \zv_v$                          |                随意                | $\| \dec (\zv_u, \zv_v) - [\Sv]_{uv} \|_2^2$ |
|  DeepWalk   | $\exp(\zv_u^\top \zv_v) / \sum_{k \in \Vcal} \exp(\zv_u^\top \zv_k)$ |           $P(v \mid u)$            |   $- [\Sv]_{uv} \log (\dec(\zv_u, \zv_v))$   |
|  node2vec   | $\exp(\zv_u^\top \zv_v) / \sum_{k \in \Vcal} \exp(\zv_u^\top \zv_k)$ |         有偏$P(v \mid u)$          |   $- [\Sv]_{uv} \log (\dec(\zv_u, \zv_v))$   |

</div>

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 多重关系预测

常见任务：知识图谱补全

<br>

此时解码器为$\dec: \Rbb^d \times \Rcal \times \Rbb^d \mapsto \Rbb_+$，$\dec(\zv_u, \tau, \zv_v)$可看作边$(u, \tau, v)$存在于图中的似然

<br>

多重关系预测的问题形式化：

$$
\begin{align*}
    \Lcal = \sum_{(u,\tau,v) \in \Ecal} l(\dec(\zv_u, \tau, \zv_v), [\Av]_{u,\tau,v})
\end{align*}
$$

其中$\Av \in \Rbb^{|\Vcal| \times |\Rcal| \times |\Vcal|}$是邻接张量

<br>

对于多重关系预测

- 相似性度量通常就是邻接张量，因为高阶的邻接张量没有物理意义
- 损失函数和解码器依然可以排列组合

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 多重关系预测

非多重关系预测中使用的损失函数$\| \dec (\zv_u, \zv_v) - [\Sv]_{uv} \|_2^2$这里不太适合

- 非多重关系预测中，$\Sv$可以为$\Av$的幂次，多值更适合看作回归问题
- 多重关系预测中，$\Sv$就是邻接张量，二值更适合看作分类问题

带负采样的交叉熵：

$$
\begin{align*}
    \Lcal = - \sum_{(u,\tau,v) \in \Ecal} \log (\dec(\zv_u, \tau, \zv_v)) - \gamma \Ebb_{w \sim P_u(\Vcal)} [\log (\sigma (-\dec(\zv_u, \tau, \zv_w))]
\end{align*}
$$

- 前者是正确预测“存在边”的对数似然
- 后者是正确预测“不存在边”的对数似然，$P_u(\Vcal)$是跟点$u$相关的概率分布

Hinge 损失：

$$
\begin{align*}
    \Lcal = \sum_{(u,\tau,v) \in \Ecal} \sum_{w \in P_u(\Vcal)} \max \{0, - \dec(\zv_u, \tau, \zv_v) + \dec(\zv_u, \tau, \zv_w) + \Delta \}
\end{align*}
$$

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 多重关系预测

解码器品种繁多，可以和前面两种损失任意搭配

<br>

RESCAL：$\dec(\zv_u, \tau, \zv_v) = \zv_u^\top \Rv_\tau \zv_v$，每种关系对应一个$\Rv_\tau \in \Rbb^{d \times d}$

<br>

平移解码

- TransE：$\dec(\zv_u, \tau, \zv_v) = - \| \zv_u + \rv_\tau - \zv_v \|$
- TransX：$\dec(\zv_u, \tau, \zv_v) = - \| g_{1,\tau} (\zv_u) + \rv_\tau - g_{2,\tau}(\zv_v) \|$
- TransH：$\dec(\zv_u, \tau, \zv_v) = - \| (\zv_u - \wv_\tau^\top \zv_u \wv_\tau ) + \rv_\tau - (\zv_v - \wv_\tau^\top \zv_v \wv_\tau ) \|$

<br>

多重线性点积 DistMult：$\dec(\zv_u, \tau, \zv_v) = \sum_{i \in [d]} [\zv_u]_i [\rv_\tau]_i [\zv_v]_i$，有对称性

<br>

复数解码

- ComplEx：$\dec(\zv_u, \tau, \zv_v) = \Re (\sum_{i \in [d]} [\zv_u]_i [\rv_\tau]_i \overline{[\zv_v]_i})$，其中$\zv_u, \tau, \zv_v \in \Cbb^d$
- RotatE：$\dec(\zv_u, \tau, \zv_v) = - \| \zv_u \odot \rv_\tau - \zv_v \|$，其中$[\rv_\tau]_i = \exp(j ~ \theta_{\tau, i})$

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 多重关系预测

可以通过对逻辑关系的表示考察解码器的能力

- 对称性：$(u, \tau, v) \in \Ecal \leftrightarrow (v, \tau, u) \in \Ecal$
- 反对称性：$(u, \tau, v) \in \Ecal \leftrightarrow (v, \tau, u) \not \in \Ecal$

<br>

DistMult 解码器仅具有对称性，TransE 仅具有反对称性：

$$
\begin{align*}
    - \| \zv_u + \rv_\tau - \zv_v \| = - \| \zv_v + \rv_\tau - \zv_u \| \Rightarrow \rv_\tau^\top (\zv_u  - \zv_v) = 0 \Rightarrow \rv_\tau = \zerov
\end{align*}
$$

- 逆性：$(u, \tau_1, v) \in \Ecal \leftrightarrow (v, \tau_2, u) \in \Ecal$
- 复合性：$(u, \tau_1, w) \in \Ecal \wedge (w, \tau_2, v) \in \Ecal \rightarrow (v, \tau_3, u) \in \Ecal$

<br>

TransE 具有复合性：

$$
\begin{align*}
    - \| \zv_u + \rv_{\tau_1} + \rv_{\tau_2} - \zv_v \| = - \| \underbrace{\zv_u + \rv_{\tau_1} - \zv_w}_{=\zerov} + \underbrace{\zv_w + \rv_{\tau_2} - \zv_v}_{=\zerov} \|
\end{align*}
$$

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 预训练 GNN

以最小化邻居重构损失预训练的 GNN 和随机初始化的 GNN 性能相仿

<br>

Deep Graph Infomax (DGI)

$$
\begin{align*}
    \Lcal = - \sum_{u \in \Vcal_\train} ( \Ebb_\Gcal \log(\overbrace{D(\zv_u, \zv_\Gcal)}^{\nearrow}) + \gamma \Ebb_{\widetilde{\Gcal}} \log (1 - \overbrace{D(\widetilde{\zv}_u, \zv_\Gcal)}^{\searrow}) )
\end{align*}
$$

- $\widetilde{\Gcal}$：原图$\Gcal$的随机扰动版
- $\zv_u$：点$u$的嵌入表示
- $\widetilde{\zv}_u$：点$u$在扰动图$\widetilde{\Gcal}$上的嵌入表示
- $D(\cdot, \cdot)$：判别当前的嵌入表示来自原图还是扰动图，通常是个神经网络

<br>

基本思想：GNN 生成的点嵌入表示在原图和扰动图上应该显著不同

<br>

DGI 也可用于监督学习设定下的辅助损失

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="进一步扩展，采用各种不同的降采样方法" -->

HEADER 效率问题

各个点独自做消息传播会产生大量重复计算，可采用矩阵形式计算

$$
\begin{align*}
    \Hv^\sup{k} = \sigma ( \Wv_\self^\sup{k} ~ \Hv^\sup{k-1} + \Wv_\neigh^\sup{k} \Hv^\sup{k-1} \Av )
\end{align*}
$$

- 整图操作非常耗内存
- 反向传播训练时也是用 GD，而不是 SGD

<br>

小批量：随机采样一些点可能得不到一个连通子图，无法做消息传播

<br>

方案：

- 先确定一小部分点，递归地加入这些点的邻居，直到得到连通图
- 设置每个点的邻居个数上界，避免产生太大的子图

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 正则化

适用于 GNN 的传统正则化

- $\ell_2$惩罚
- 随机丢弃
- 层归一化

<br>

各层权值共享

- 所有$\aggregate (\cdot)$和$\update (\cdot)$均共享权值
- 适用于超过 6 层的 GNN

<br>

边随机丢弃：于 GAT 中首次提出

- 训练时随机删除一些边
- 适用于知识图谱

FOOTER3 图神经网络导论 图神经网络 tengzhang@hust.edu.cn
