---
presentation:
  transition: "none"
  enableSpeakerNotes: true
  margin: 0
---

@import "../common/css/font-awesome-4.7.0/css/font-awesome.css"
@import "../common/css/zhangt-solarized.css"
@import "css/GNN.css"

<!-- slide data-notes="" -->

GNN-HEADER 特征变换 核函数

核函数$\kappa(\cdot, \cdot)$是双变量对称函数，即$\kappa(\xv, \zv) = \kappa(\zv, \xv)$，常见的有：

- 线性核$\kappa (\xv, \zv) = \xv^\top \zv$，相当于用了恒等核映射$\phi(\xv) = \xv$
- 多项式核$\kappa (\xv, \zv) = (\xv^\top \zv + k)^d$，$k = 0$则为齐次多项式核，$d \in \Zbb_+$
- 高斯核$\kappa (\xv, \zv) = \exp (- \| \xv - \zv \|^2 / 2 \sigma^2)$，$\sigma > 0$称为高斯核的带宽 (width)
- 拉普拉斯核$\kappa (\xv, \zv) = \exp (- \| \xv - \zv \| / \sigma)$，$\sigma > 0$
- Sigmoid 核$\kappa (\xv, \zv) = \tanh (\beta \xv^\top \zv + \theta)$，$\tanh$为双曲正切函数，$\beta > 0$，$\theta < 0$

<br>

将 PCA 中的样本$\xv$用$\phi(\xv)$替代，就得到了核 PCA，先升维再降维

<br>

PCA：$\max_{\|\wv\|_2^2 = 1} \wv^\top \Xv^\top \Xv \wv$ → 核 PCA：$\max_{\|\wv\|_2^2 = 1} \wv^\top \phi(\Xv)^\top \phi(\Xv) \wv$

<br>

其中$\Xv = \begin{bmatrix} \xv_1^\top \\ \vdots \\ \xv_m^\top \end{bmatrix}$、$\phi(\Xv) = \begin{bmatrix} \phi(\xv_1)^\top \\ \vdots \\ \phi(\xv_m)^\top \end{bmatrix}$，注意两个$\wv$的维度不一样

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 特征变换 核 PCA

问题：如何让模型中只出现内积$\phi(\xv_i)^\top \phi(\xv_j)$的形式？

对$\wv$做正交分解$\wv = \sum_{i \in [m]} \alpha_i \phi(\xv_i) + \vv = \phi(\Xv)^\top \alphav + \vv$，其中

$$
\begin{align*}
    \vv \perp \span \{ \phi(\xv_1), \ldots, \phi(\xv_m) \} ~ \Longrightarrow ~ \phi(\Xv) \vv = \zerov
\end{align*}
$$

于是

$$
\begin{align*}
    \|\wv\|_2^2 & = \alphav^\top \phi(\Xv) \phi(\Xv)^\top \alphav + \vv^\top \vv = \alphav^\top \Kv \alphav + \vv^\top \vv \\
    \phi(\Xv) \wv & = \phi(\Xv) (\phi(\Xv)^\top \alphav + \vv) = \phi(\Xv) \phi(\Xv)^\top \alphav = \Kv \alpha
\end{align*}
$$

其中$\Kv = \phi(\Xv) \phi(\Xv)^\top = \begin{bmatrix} \phi(\xv_1)^\top \phi(\xv_1) & \cdots & \phi(\xv_1)^\top \phi(\xv_m) \\ \vdots & \ddots & \vdots \\ \phi(\xv_m)^\top \phi(\xv_1) & \cdots & \phi(\xv_m)^\top \phi(\xv_m) \end{bmatrix}$为核矩阵

核 PCA 可重写为$\max_{\alphav, \vv} \alphav^\top \Kv \Kv \alphav, \quad \st ~ \alphav^\top \Kv \alphav + \vv^\top \vv = 1$

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 特征变换 核 PCA

核 PCA：$\max_{\alphav, \vv} \alphav^\top \Kv \Kv \alphav, \quad \st ~ \alphav^\top \Kv \alphav + \vv^\top \vv = 1$

设最优解为$(\alphav_\star, ~ \vv_\star)$，下面说明$\vv_\star = \zerov$

- 若$\vv_\star^\top \vv_\star = c > 0$，则$\alphav_\star^\top \Kv \alphav_\star = 1 - c < 1$
- $(\alphav_0 = 1 / \sqrt{1-c} ~ \alphav_\star, ~ \vv_0 = \zerov)$也是一组可行解
- 显然$\alphav_0^\top \Kv \Kv \alphav_0 = \alphav_\star^\top \Kv \Kv \alphav_\star / (1-c) > \alphav_\star^\top \Kv \Kv \alphav_\star$，这与$\alphav_\star$的最优性矛盾

<br>

核 PCA 的最终形式为：

$$
\begin{align*}
    \max_\alphav ~ \alphav^\top \Kv \Kv \alphav, \quad \st ~ \alphav^\top \Kv \alphav = 1
\end{align*}
$$

通过拉格朗日乘子法求得系数$\alphav$后，样本$\xv_j$在成分$\wv$上的投影为

$$
\begin{align*}
    \wv^\top \phi(\xv_j) = \sum_{i \in [m]} \alpha_i \phi(\xv_i)^\top \phi(\xv_j) = \sum_{i \in [m]} \alpha_i \kappa (\xv_i, \xv_j)
\end{align*}
$$

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn
