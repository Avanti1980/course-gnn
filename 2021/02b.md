---
presentation:
  transition: "none"
  enableSpeakerNotes: true
  margin: 0
---

@import "../common/css/font-awesome-4.7.0/css/font-awesome.css"
@import "../common/css/zhangt-solarized.css"
@import "css/GNN.css"

<!-- slide data-notes="" -->
<div class="header"><img class="hust"></div>

<div class="bottom15"></div>

# 图神经网络导论

<hr class="width18">

## 机器学习

<div class="bottom5"></div>

### 计算机科学与技术学院 &nbsp; &nbsp; 张腾

<br>

#### tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 特征变换

该步是模型学习前的最后一步，亦有将该步与模型学习融合的做法

<br>

<div class="invis" markdown=1>

当部分特征冗余甚至有害时，挑选出有用的特征子集

- 去除低方差特征，特别是那些在所有样本上取值均不变的特征
- 先计算 F 检验值、卡方检验值、互信息、线性相关性等统计量，然后据此设立阈值选择特征
- 通过 PCA、随机投影等降维技术浓缩现有特征
- 引入$\ell_1$等稀疏范数作为约束，将选择特征与模型学习合二为一

</div>

<br>

当特征稀缺时，利用现有特征构造新的特征

- 凭经验显式构造：$\xv = [x_1; x_2] \xrightarrow{\Rbb^2 \mapsto \Rbb^6} \xvt = [x_1^2; x_2^2; \sqrt{2} x_1 x_2; \sqrt{2} x_1; \sqrt{2} x_2; 1]$
- 利用核函数$\kappa(\xv, \zv) = \langle \phi(\xv), \phi(\zv) \rangle$隐式构造，其中$\phi: \Rbb^d \mapsto \Hbb$是核映射，$\langle \cdot, \cdot \rangle$是空间$\Hbb$中的内积，代表性方法为核方法
- 利用非线性函数复合$f_n ( f_{n-1} ( \cdots f_2 (f_1 (\xv))))$，代表性方法为神经网络

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 特征变换

该步是模型学习前的最后一步，亦有将该步与模型学习融合的做法

<br>

<div class="invis" markdown=1>

当部分特征冗余甚至有害时，挑选出有用的特征子集

- 去除低方差特征，特别是那些在所有样本上取值均不变的特征
- 通过方差分析、卡方检验、互信息、线性相关性等统计量选择特征
- 通过 PCA、随机投影等降维技术浓缩现有特征
- 引入$\ell_1$等稀疏范数作为约束，将选择特征与模型学习合二为一

</div>

<br>

当特征稀缺时，利用现有特征构造新的特征

- 凭经验显式构造：$\xv = [x_1; x_2] \xrightarrow{\Rbb^2 \mapsto \Rbb^6} \xvt = [x_1^2; x_2^2; x_1 x_2; x_1; x_2; 1]$
- 利用核函数$\kappa(\xv, \zv) = \langle \phi(\xv), \phi(\zv) \rangle$隐式构造，其中$\phi: \Rbb^d \mapsto \Hbb$是核映射，$\langle \cdot, \cdot \rangle$是空间$\Hbb$中的内积，决策函数为$f(\phi(\zv))$，代表性方法为核方法
- 利用非线性函数复合，决策函数为$f_n ( f_{n-1} ( \cdots f_2 (f_1 (\xv))))$，代表性方法为深度神经网络

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn
