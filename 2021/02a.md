---
presentation:
  transition: "none"
  enableSpeakerNotes: true
  margin: 0
---

@import "../common/css/font-awesome-4.7.0/css/font-awesome.css"
@import "../common/css/zhangt-solarized.css"
@import "css/GNN.css"

<!-- slide data-notes="因为有超过一半的同学没学过机器学习，所以我打算花两次课的时间做个机器学习的扫盲，顺便把一些基本概念，如损失函数、正则化什么的讲一下，这些概念即便到了图神经网络里也是通用的" -->
<div class="header"><img class="hust"></div>

<div class="bottom15"></div>

# 图神经网络导论

<hr class="width18">

## 机器学习 上

<div class="bottom5"></div>

### 计算机科学与技术学院 &nbsp; &nbsp; 张腾

<br>

#### tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="这张图算是人工智能的常见黑话大集合，大家如果稍微关注些人工智能的话，应该会经常听到这些词，我这里给大家捋一下它们之间的关系，最大的概念叫人工智能<br>监督信息和模型假设并不是并列关系的，只是看待机器学习的不同视角，因此有半监督支持向量机这种东西" -->

GNN-HEADER 大纲

@import "../dot/outline.dot"

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide data-notes="就先从最大的人工智能说起吧" -->

GNN-HEADER 大纲

@import "../dot/outline-ai.dot"

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="控制论之父维纳在他的大作《控制论》<br><br>讲完后提一下，现在我们国家也高度重视人工智能的发展，因为1760年第一次工业革命，1870年第二次工业革命我国都完全没赶上，互联网和移动互联网引发第三次工业革命我们国家也只赶上个末班车，而西方国家确立对非西方的绝对优势就是这三次工业革命，所以对于人工智能技术为核心推动的第四次工业革命是绝对重视，所以这里打个广告，大家现在上车还来得及<br><br>扯远了" -->

GNN-HEADER 背景

<br>

维纳 (Wiener) &emsp; 《控制论》：

<br>

> 第一次工业革命：用某种机器来减轻甚至代替<span class="blue">体力</span>劳动<br>
> 上世纪中叶：用某种新型机器来减轻甚至代替某些<span class="blue">脑力</span>劳动

</br>

关键：让机器具有人类的智能

</br>

问题：什么是“智能”？

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="智能这一说法最早是图灵在他1950年的论文里提出的，后来被人们称为图灵测试<br><br>学习指学习新东西、探索未知的能力，感知处理看和听，认知是思考和推理" -->

GNN-HEADER 起源

<div class="multi_column bottom-4">
    <div class="width22 left0 right0">
        <p>“智能”一词难以直接定义</p><br>
        <p>图灵 (Turing) 1950 年的论文<br>《Computing Machinery and Intelligence》</p><br>
        <p><span class="blue">图灵测试</span>：一个人在不接触对方的情况下，通过一种特殊的方式，和对方进行一系列的问答，如果在相当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的</p>
    </div>
    <div class="width10 left0 right2 top4">
        <img src="img/turing.jpg" class="width30 top2">
    </div>
</div>

要想通过图灵测试，机器得具备多种能力

- 学习：机器学习
- 感知：计算机视觉，语音识别
- 认知：自然语言处理，知识表示

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="图中的5位是06年会议50周年时还建在的5位，左一是摩尔，右二赛弗里奇，右一是所罗门诺夫<br><br>香农是被麦卡锡拉过去站台的，除香农都得过图灵奖，C罗奖给梅西，梅西也不会要<br><br>1971年麦卡锡获得图灵奖，发明了第一个函数式程序语言Lisp，函数式编程更加强调程序执行的结果而非执行的过程，倡导利用若干简单的执行单元让计算结果不断渐进，逐层推导复杂的运算，而不是设计一个复杂的执行过程<br><br>明斯基写过一本书，叫感知机<br><br>西蒙是中科院外籍院士，9个博士学位，中文名司马贺<br><br>纽厄尔一直是西蒙的合作者，曾在美国智库兰德公司供职" -->

GNN-HEADER 元年

达特茅斯 (Dartmouth) 会议

- 时间：1956 年
- 地点：达特茅斯学院
- 人物：香农 (Shannon)、麦卡锡 (McCarthy)、明斯基 (Minsky)、西蒙 (Simon)、纽厄尔 (Newell) 等十人
- 事件：讨论用机器模拟人的智能

<br>

<div class="multi_column top_2">
    <img src="../common/img/birth-school.jpg" width=425px height=277px style="margin-left:2.5rem">
    <img src="../common/img/birth-people.jpg" width=425px height=277px style="margin-right:2.5rem;margin-left:auto">
</div>

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="人工智能的发展大致可以分成三个阶段，" -->

GNN-HEADER 发展

@import "../mermaid/ai.mermaid"

秽土转生

- 推理：反绎学习，图神经网络
- 知识：知识图谱，图神经网络

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide data-notes="我个人猜测原因是数学原理讲的是公理集合论 当然是罗素的 不是zfc 天生符号化" -->

GNN-HEADER 推理期

<div class="sparse">

机器擅长固定套路的计算 vs. 人类擅长妙手偶得的推理

符号主义：<span class="blue">智能 = 逻辑推理</span>

</div>

西蒙和纽厄尔设计了“逻辑理论家”程序

- 1952 年，逻辑理论家证明了 《数学原理》 中的 38 条定理
- 1963 年，证明了全部 52 条定理，其中定理 2.85 的证明比原书作者更巧妙
- 西蒙和纽厄尔获得了 1975 年的图灵奖

<div class="sparse">

衰退：十万步无法证明“两个连续函数之和还是连续函数”

</div>

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 符号主义

根据以下事实判别谁说了实话

- $A$：$B$和$C$都是说谎者
- $B$：$A$和$C$都是说谎者
- $C$：$A$和$B$中至少有一个说谎者

<div class="threelines resolution" markdown="1">

|   公式   |          $p \rightarrow q$           | $\Longleftrightarrow$ |                   $\neg p \vee q$                    |
| :------: | :----------------------------------: | :-------------------: | :--------------------------------------------------: |
| **条件** | $A \rightarrow \neg B \wedge \neg C$ | $\Longleftrightarrow$ | $1.~\neg A \vee \neg B, \quad 2.~\neg A \vee \neg C$ |
|          |    $\neg A \rightarrow B \vee C$     | $\Longleftrightarrow$ |                 $3.~A \vee B \vee C$                 |
|          | $B \rightarrow \neg A \wedge \neg C$ | $\Longleftrightarrow$ |               $4.~\neg B \vee \neg C$                |
|          |    $\neg B \rightarrow A \vee C$     | $\Longleftrightarrow$ |                 $3.~A \vee B \vee C$                 |
|          |  $C \rightarrow \neg A \vee \neg B$  | $\Longleftrightarrow$ |         $5.~\neg A \vee \neg B \vee \neg C$          |
|          |   $\neg C \rightarrow A \wedge B$    | $\Longleftrightarrow$ |           $6.~A \vee C, \quad 7.~B \vee C$           |
| **归结** | $1 + 7 \rightarrow 8.~\neg A \vee C$ | $\Longleftrightarrow$ |                     $C$说了实话                      |

</div>

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="从推理期得到的教训是" -->

GNN-HEADER 知识期

<div class="sparse">

教训：光有逻辑推理远远不够，机器得拥有知识

信仰：“知识就是力量”，<span class="blue">智能 = 知识 + 逻辑推理</span>

</div>

专家系统 = 知识库 + 推理机

- 在特定领域内具有专家水平解决问题能力的程序系统
- 第一个成功的专家系统 DENDRAL 于 1968 年问世
- 知识工程之父费根鲍姆 (Feigenbaum) 获得了 1994 年的图灵奖

<div class="sparse">

衰退：人工构建知识库成本太高，知识获取困难

</div>

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 动物识别专家系统

@import "../dot/reasoning.dot"

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide data-notes="对于人类的很多智能行为(比如语言理解、图像理解等) 我们很难知道其中的原理 也无法描述出这些智能行为背后的“知识” <br><br> 因此 我们也很难通过知识和推理的方式来实现这些行为的智能系统。为了解决这类问题,研究者开始将研究重点转向让计算机从数据中自己学习" -->

GNN-HEADER 学习期

基本想法：让<span class="blue">机器</span>从数据中自动<span class="blue">学习</span>得到某种知识 (规律)

</br>

基本流程：

@import "../dot/ml-old.dot"

- 原始数据：图片、视频、文本、语音、……
- 特征提取：选取有用特征，如对西瓜选取色泽、根蒂、敲声、纹理、触感
- 特征处理：无序类别特征转换为数值特征，处理缺失特征，特征标准化等
- 特征变换：对提取的特征作变换生成新的更有效的特征
- <span class="blue">模型学习</span>：机器学习最核心的部分，学习一个特征到类别标记的映射

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 特征提取 以文本为例

<span class="blue">词袋模型</span> (bag-of-words)：文本是单词的集合，单词间独立、无序

所有文本全部$d$个不同的单词构成词典，每个文本提取$d$个特征

若词典第$i$个词在当前文本中出现过，则其第$i$个特征为$1$，否则为$0$

```python {.line-numbers}
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
document1 = "I have a pen, I have an apple, apple pen."
document2 = "I have a pen, I have pineapple, pineapple pen."
cv = CountVectorizer(lowercase=False, token_pattern='\w+', binary=True)
model = cv.fit_transform([document1, document2])
pd.DataFrame(model.toarray(), columns=cv.get_feature_names_out())
```

<div class="threelines leftfirstcolumn top-2">

| Dictionary |  I  |  a  | an  | apple | have | pen | pineapple |
| :--------: | :-: | :-: | :-: | :---: | :--: | :-: | :-------: |
| document1  |  1  |  1  |  1  |   1   |  1   |  1  |     0     |
| document2  |  1  |  1  |  0  |   0   |  1   |  1  |     1     |

</div>

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 特征提取 以文本为例

<span class="blue">词袋模型</span> (bag-of-words)：文本是单词的集合，单词间独立、无序

所有文本全部$d$个不同的单词构成词典，每个文本提取$d$个特征

若词典第$i$个词在当前文本中出现了$k$次，则其第$i$个特征为$k$

```python {.line-numbers}
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
document1 = "I have a pen, I have an apple, apple pen."
document2 = "I have a pen, I have pineapple, pineapple pen."
cv = CountVectorizer(lowercase=False, token_pattern='\w+')
model = cv.fit_transform([document1, document2])
pd.DataFrame(model.toarray(), columns=cv.get_feature_names_out())
```

<div class="threelines leftfirstcolumn top-2">

| Dictionary |  I  |  a  | an  | apple | have | pen | pineapple |
| :--------: | :-: | :-: | :-: | :---: | :--: | :-: | :-------: |
| document1  |  2  |  1  |  1  |   2   |  2   |  2  |     0     |
| document2  |  2  |  1  |  0  |   0   |  2   |  2  |     2     |

</div>

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="这里要说一下啥是 l1 l2" -->

GNN-HEADER 特征提取 以文本为例

<span class="blue">词袋模型</span> (bag-of-words)：文本是单词的集合，单词间独立、无序

所有文本全部$d$个不同的单词构成词典，每个文本提取$d$个特征

词频 - 逆文本频率 (tf - idf) 特征：对当前文本重要的单词必然

- 在当前文本中出现的频率高，即词频 (term frequency, tf) 高
- 在其他文本中出现的频率低，即逆文本频率 (inverse document frequency, idf) 高

tf = 单词在当前文本中出现的次数 / 当前文本的总词数

idf = ln ((全部文本数 + C) / (包含该词的总文本数 + C)) + 1

- C = 0，若词典包含从未在任何文本中出现的词，会有分母为零的问题
- C = 1，sklearn 中默认采用的平滑版本，相当于额外有一个包含所有词的文本

tf - idf 特征 = normalize (tf × idf)，将 tf 和 idf 相乘后再归一化

- $\ell_1$归一化，tf × idf / sum (tf × idf)，即归一化成概率分布
- $\ell_2$归一化，tf × idf / sqrt(sum ([tf × idf]^2))，即归一化成模为 1 的向量

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 特征提取 以文本为例

<br>

```python {.line-numbers}
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
document1 = "I have a pen, I have an apple, apple pen."
document2 = "I have a pen, I have pineapple, pineapple pen."
tv = TfidfVectorizer(lowercase=False, token_pattern='\w+',
                     norm='l1', smooth_idf=False) # l1归一化 idf不平滑
model = tv.fit_transform([document1, document2])
pd.DataFrame(model.toarray(), columns=cv.get_feature_names_out())
```

<div class="threelines top-2">

|          |     I      |     a      |     an     |   apple    |    have    |    pen     | pineapple  |
| :------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------: |
|    tf    |   2 / 10   |   1 / 10   |   1 / 10   |   2 / 10   |   2 / 10   |   2 / 10   |     0      |
|    ^     |   2 / 9    |   1 / 9    |     0      |     0      |   2 / 9    |   2 / 9    |   2 / 9    |
|   idf    | ln (1) + 1 | ln (1) + 1 | ln (2) + 1 | ln (2) + 1 | ln (1) + 1 | ln (1) + 1 | ln (2) + 1 |
| tf - idf |  0.165571  |  0.082785  |  0.140168  |  0.280335  |  0.165571  |  0.165571  |  0.000000  |
|    ^     |  0.192561  |  0.096281  |  0.000000  |  0.000000  |  0.192561  |  0.192561  |  0.326035  |

</div>

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER 类别特征 → 数值特征

<div class="threelines watermelon">

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度  | 含糖率 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: | :----: | :--: |
|  1   | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 | 0.460  |  是  |
|  2   | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.774 | 0.376  |  是  |
|  3   | 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.666 | 0.091  |  否  |
|  4   | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 0.245 | 0.057  |  否  |

</div>

三种重编码方式：

- 序数编码 (ordinal encoding)：清晰 - 0、稍糊 - 1、模糊 - 2，需类别特征本身有序，否则若青绿 - 0、乌黑 - 1、浅白 - 2，为何 | 青绿 - 浅白 | > | 乌黑 - 浅白 | ？
- 独热编码 (one-hot encoding)：青绿 - 001、乌黑 - 010、浅白 - 100，一碗水端平，所有取值距离相等，但若取值很多码会很长，且不适应动态出现的新取值
- 哈希编码 (hash encoding)：用哈希函数将任意输入映射到有限整数范围，码长固定，也能适应动态出现的新取值，但可能存在信息丢失

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 类别特征 → 数值特征

```python {.line-numbers}
import numpy as np
from sklearn.preprocessing import LabelBinarizer, OneHotEncoder
X = np.array([
    [1, '青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, 0.460],
    [2, '乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.774, 0.376],
    [3, '乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', 0.666, 0.091],
    [4, '浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', 0.245, 0.057],
])
y = np.array(['是', '是', '否', '否']) # 类别标记只有两种取值
print(LabelBinarizer().fit_transform(y).squeeze())
[1 1 0 0]

enc = OneHotEncoder()
print(enc.fit_transform(X[:, 1:7]).toarray()) # 对6个类别特征采用独热编码
[[0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]
 [1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1.]
 [0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.]]

print(enc.get_feature_names_out()) # 独热编码对应的原始特征
['x0_乌黑' 'x0_浅白' 'x0_青绿' 'x1_硬挺' 'x1_稍蜷' 'x1_蜷缩' 'x2_沉闷' 'x2_浊响'
 'x2_清脆' 'x3_模糊' 'x3_清晰' 'x3_稍糊' 'x4_凹陷' 'x4_平坦' 'x4_稍凹' 'x5_硬滑']
```

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide data-notes="有时会因为特殊原因，特征不是完整的，比如医院的病人数据，病人不可能把你的所有检查都来一遍，都是能省则省 <br><br> 本来机器学习的目的是要找到特征到类别标记的映射，对于缺失特征，可以将其先看作要预测的类别标记，先学一个无缺失特征到有缺失特征的映射，利用这个映射先将缺失的特征都补上，然后再学习类别标记 <br><br> 比如图里这个例子，含糖率有缺失，我就先用除含糖率外的特征学习一个到含糖率的映射，这是一个回归问题" -->

GNN-HEADER 特征缺失处理

<div class="threelines watermelon">

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度  | 含糖率 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: | :----: | :--: |
|  1   | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 |   -    |  是  |
|  2   | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.774 | 0.376  |  是  |
|  3   | 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.666 | 0.091  |  否  |
|  4   | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 0.245 | 0.057  |  否  |

</div>

删除：直接删除有特征缺失的样本，简单粗暴，信息损失

补全：

- 用其他未缺失该特征的样本计算平均数、中位数、众数填充，人为引入噪声
- 用不存在缺失的其它特征<span class="blue">学习并预测</span>缺失特征的取值，若两者之间无关？
- 将“缺失”本身作为一种特征取值

忽略：采用对缺失特征不敏感的学习模型，如决策树

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER 特征标准化

也称归一化，旨在<span class="blue">消除不同特征间的量纲影响</span>

<br>

min - max 放缩：也称为离差标准化，将原始特征线性变换到 [0, 1] 区间

$$
\begin{align*}
    x \leftarrow \frac{x - x_\min}{x_\max - x_\min} \quad \wedge \quad 0 \leftarrow x_\min \quad \wedge \quad 1 \leftarrow x_\max
\end{align*}
$$

最大值标准化：除以该特征的绝对值最大值

$$
\begin{align*}
    x \leftarrow \frac{x}{\max_{i \in [m]} |x_i|} \in [-1,1] \quad \wedge \quad (-1 \leftarrow x_\min \quad \vee \quad 1 \leftarrow x_\max)
\end{align*}
$$

标准差标准化：经过处理的特征近似符合标准正态分布$\Ncal(0,1)$

$$
\begin{align*}
    x \leftarrow \frac{x - \mu}{\sigma}, \quad x \leftarrow \frac{x - x_{\text{median}}}{\sum_{i \in [m]} |x_i - x_{\text{median}}| / m}
\end{align*}
$$

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 特征标准化

```python {.line-numbers}
import numpy as np
from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler

X = np.array([
    [1, '青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, 0.460],
    [2, '乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.774, 0.376],
    [3, '乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', 0.666, 0.091],
    [4, '浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', 0.245, 0.057],
])

print(MinMaxScaler().fit_transform(X[:, 7:9]))
[[0.85444234, 1.        ],
 [1.        , 0.79156328],
 [0.79584121, 0.08436725],
 [0.        , 0.        ]]

print(MaxAbsScaler().fit_transform(X[:, 7:9]))
[[0.9005168 , 1.        ],
 [1.        , 0.8173913 ],
 [0.86046512, 0.19782609],
 [0.31653747, 0.12391304]]
```

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 特征标准化

```python {.line-numbers}
import numpy as np
from sklearn.preprocessing import scale

X = np.array([
    [1, '青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', 0.697, 0.460],
    [2, '乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', 0.774, 0.376],
    [3, '乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', 0.666, 0.091],
    [4, '浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', 0.245, 0.057],
])

x = scale(X[:, 7:9])
print(x)
[[ 0.49236904  1.22314674]
 [ 0.86589038  0.74303307]
 [ 0.34199032 -0.88592404]
 [-1.70024974 -1.08025576]]

print(x.mean(axis=0))
[-1.11022302e-16 -1.66533454e-16]

print(x.std(axis=0))
[1. 1.]
```

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide data-notes="并非所有的特征都对后面的模型学习有用" -->

GNN-HEADER 特征变换

该步是模型学习前的最后一步，亦有将该步与模型学习融合的做法

<br>

当部分特征冗余甚至有害时，挑选出有用的特征子集，降维

- 过滤掉与学习目标无关的特征，例如西瓜的编号显然与其好坏无关
- 过滤掉低方差特征，尤其是那些在所有样本上取值均相同的特征 (方差为零)

<br>

计算特征与类别标记的线性相关性、互信息等，再划定阈值遴选特征

利用$\ell_1$范数最小化等正则化技术，遴选特征与模型学习合二为一

<br>

当特征稀缺时，利用现有特征构造新的特征，升维

- 凭经验显式构造：$\xv = [x_1; x_2] \xrightarrow{\Rbb^2 \mapsto \Rbb^6} \xvt = [x_1^2; x_2^2; x_1 x_2; x_1; x_2; 1]$
- 利用核函数$\kappa(\xv, \zv) = \langle \phi(\xv), \phi(\zv) \rangle$隐式构造，其中$\phi: \Rbb^d \mapsto \Hbb$是核映射，$\langle \cdot, \cdot \rangle$是空间$\Hbb$中的内积，决策函数为$f(\phi(\zv))$，代表性方法为核方法
- 利用非线性函数复合，决策函数为$f_n ( f_{n-1} ( \cdots f_2 (f_1 (\xv))))$，代表性方法为深度神经网络

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="并非所有的特征都对后面的模型学习有用" -->

GNN-HEADER 特征变换

该步是模型学习前的最后一步，亦有将该步与模型学习融合的做法

<br>

当部分特征冗余甚至有害时，挑选出有用的特征子集，降维

- 过滤掉与学习目标无关的特征，例如西瓜数据集中的编号，其与瓜的好坏无关
- 过滤掉低方差特征，特别是那些在所有样本上取值均不变的特征

计算特征与类别标记的线性相关性、互信息等，再划定阈值遴选特征

利用$\ell_1$范数最小化等正则化技术，遴选特征与模型学习合二为一

<br>

当特征稀缺时，利用现有特征构造新的特征，升维

- 凭经验显式构造：$\xv = [x_1; x_2] \xrightarrow{\Rbb^2 \mapsto \Rbb^6} \xvt = [x_1^2; x_2^2; x_1 x_2; x_1; x_2; 1]$
- 利用核函数$\kappa(\xv, \zv) = \langle \phi(\xv), \phi(\zv) \rangle$隐式构造，其中$\phi: \Rbb^d \mapsto \Hbb$是核映射，$\langle \cdot, \cdot \rangle$是空间$\Hbb$中的内积，决策函数为$f(\phi(\zv))$，代表性方法为核方法
- 利用非线性函数复合，决策函数为$f_n ( f_{n-1} ( \cdots f_2 (f_1 (\xv))))$，代表性方法为深度神经网络

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn
