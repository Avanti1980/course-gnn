---
presentation:
  transition: "none"
  enableSpeakerNotes: true
  margin: 0
---

@import "../common/css/font-awesome-4.7.0/css/font-awesome.css"
@import "../common/css/zhangt-solarized.css"
@import "css/GNN.css"

<!-- slide data-notes="" -->
<div class="header"><img class="hust"></div>

<div class="bottom15"></div>

# 图神经网络导论

<hr class="width50">

## 神经网络

<div class="bottom5"></div>

### 计算机科学与技术学院 &nbsp; &nbsp; 张腾

<br>

#### tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 发展史

@import "../mermaid/nn.mermaid"

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 神经元

神经网络的基本结构称为神经元

<br>

单个神经元对应运算$y = h(\wv^\top \xv + b)$，其中$h$是非线性<span class="blue">激活函数</span>

<div class="sparse top10 left10">

### 下图激活函数$h = \sgn(\cdot)$

</div>

<img src="../tikz/neuron.svg" class="width45 left20 top-12">

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 神经网络

将神经元广泛并行互连就构成了神经网络

@import "../dot/nn.dot" {.center}

<div></div>

只要存在隐藏层，模型就拥有了非线性预测能力

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 形式化

$L$：神经网络的层数

$n_l$：第$l$层神经元的个数

$h_l(\cdot)$：第$l$层的激活函数

$\Wv_l \in \Rbb^{n_l \times n_{l-1}}$：第$l-1$层到第$l$层的权重矩阵

$\bv_l \in \Rbb^{n_l}$：第$l$层的偏置 (截距)

$\zv_l \in \Rbb^{n_l}$：第$l$层神经元的输入

$\av_l \in \Rbb^{n_l}$：第$l$层神经元的输出

<br>

第$l$层的计算过程：$\zv_l = \Wv_l \av_{l-1} + \bv_l$，$\av_l = h_l (\zv_l)$

<br>

整个网络：$\xv = \av_0 \xrightarrow{\Wv_1,\bv_1} \zv_1 \xrightarrow{h_1} \av_1 \xrightarrow{\Wv_2,\bv_2} \cdots \xrightarrow{\Wv_L,\bv_L} \zv_L \xrightarrow{h_L} \av_L = \hat{\yv}$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 激活函数

最早的 MP 模型采用阶跃函数$1_{z \geq 0}$作为激活函数

<br>

改进方向：

- 连续并几乎处处可导，可以高效计算
- 导数的值域在合适的范围内，否则影响用梯度下降进行训练

<br>

常见的有

- Sigmoid 型：Logistic 函数，Tanh 函数
- ReLU，带泄漏的 ReLU，带参数的 ReLU，ELU，Softplus
- Swish 函数
- Maxout 单元

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER Sigmoid 型

<img src="../python/Sigmoid.svg" class="width60 center top6">

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER Logistic 函数

将$\Rbb$<span class="blue">挤压</span>到$[0,1]$，输出拥有概率意义：

$$
\begin{align*}
    \sigma(z) = \frac{1}{1 + \exp (-z)} = \begin{cases}
        1 & z \rightarrow \infty \\
        0 & z \rightarrow -\infty
    \end{cases}
\end{align*}
$$

<br>

Logistic 函数连续可导，在<span class="blue">零处导数最大</span>

$$
\begin{align*}
    \sigma'(z) & = - \frac{- \exp (-z)}{(1 + \exp (-z))^2} = \frac{1}{1 + \exp (-z)} \frac{\exp (-z)}{1 + \exp (-z)} \\
    & = \sigma(z) (1 - \sigma(z)) \leq \left( \frac{\sigma(z) + 1 - \sigma(z)}{2} \right)^2 \\
    & = \frac{1}{4}
\end{align*}
$$

均值不等式等号成立的条件是$\sigma(z) = 1 - \sigma(z)$，即$z = 0$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER Tanh 函数

将$\Rbb$<span class="blue">挤压</span>到$[-1,1]$，<span class="blue">输出零中心化</span>，Logistic 函数的放大平移

$$
\begin{align*}
    \tanh(z) & = \frac{\exp(z) - \exp(-z)}{\exp(z) + \exp(-z)} = \frac{1 - \exp(-2z)}{1 + \exp(-2z)} \\
    & = 2 \sigma(2z) - 1 \\
    & = \begin{cases}
        1 & z \rightarrow \infty \\
        -1 & z \rightarrow -\infty
    \end{cases}
\end{align*}
$$

<br>

性质：

- 连续可导$\tanh'(z) = 2 (\sigma(2z))' = 4 \sigma(2z) (1 - \sigma(2z))$，在$z = 0$处导数最大
- 输出零中心化使得后一层的输入$\wv^\top \av + \bv$也在零附近，而 Tanh 函数在零处导数最大，梯度下降更新效率较高，Logistic 函数输出恒为正，会进一步减慢梯度下降的收敛速度

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="梯度消失之后在讲了BP之后会细讲 <br><br> 死亡relu在下一页" -->

HEADER ReLU

ReLU 全称叫修正线性单元 (**re**ctified **l**inear **u**nit)，定义为

$$
\begin{align*}
    \relu(z) = \max \{ 0, z \} = \begin{cases}
        z & z \geq 0 \\ 0 & z < 0
    \end{cases}
\end{align*}
$$

<br>

优点

- 计算只涉及加法、乘法和比较操作，非常高效
- 生物学解释：单侧抑制，宽兴奋边界，稀疏兴奋
- 在$z > 0$时导数恒为$1$，缓解了<span class="blue">梯度消失</span>问题

<br>

缺点

- 输出非零中心化，对下一层不友好
- 死亡 ReLU 问题：对异常值特别敏感

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 死亡 ReLU 问题

由链式法则有

$$
\begin{align*}
    \frac{\partial \relu(\wv^\top \xv + b)}{\partial \wv} & = \frac{\partial \relu(\wv^\top \xv + b)}{\partial (\wv^\top \xv + b)} \frac{\partial (\wv^\top \xv + b)}{\partial \wv} \\
    & = \frac{\partial \max \{ 0, \wv^\top \xv + b \}}{\partial (\wv^\top \xv + b)} \xv^\top \\
    & = 1_{\wv^\top \xv + b \geq 0} \xv^\top
\end{align*}
$$

如果第一个隐藏层中的某个神经元的权重向量$\wv$初始化不当，使得对任意$\xv$有$\wv^\top \xv + b < 0$，那么其关于$\wv$的梯度将为零，在以后的训练过程中$\wv$永远不会被更新

<br>

方案：带泄漏的 ReLU，带参数的 ReLU，ELU，Softplus

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER ReLU 变体

带泄漏的 ReLU：当$\wv^\top \xv + b < 0$时也有非零梯度

$$
\begin{align*}
    \lrelu(z) & = \begin{cases}
        z & z \geq 0 \\ \gamma z & z < 0
    \end{cases} \\
    & = \max \{ 0, z \} + \gamma \min \{ 0, z \} \overset{\gamma < 1}{=} \max \{ z, \gamma z \}
\end{align*}
$$

其中斜率$\gamma$是一个很小的常数，比如$0.01$

<br>

带参数的 ReLU：斜率$\gamma_i$可学习

$$
\begin{align*}
    \prelu(z) & = \begin{cases}
        z & z \geq 0 \\ \gamma_i z & z < 0
    \end{cases} \\
    & = \max \{ 0, z \} + \gamma_i \min \{ 0, z \}
\end{align*}
$$

可以不同神经元具有不同的参数，也可以一组神经元共享一个参数

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER ReLU 变体

ELU 全称叫指数线性单元 (**e**xponential **l**inear **u**nit)

$$
\begin{align*}
    \elu(z) & = \begin{cases}
        z & z \geq 0 \\ \gamma (\exp(z) - 1) & z < 0
    \end{cases} \\
    & = \max \{ 0, z \} + \min \{ 0, \gamma (\exp(z) - 1) \}
\end{align*}
$$

<div class="bottom4"></div>

Softplus 函数可以看作 ReLU 的平滑版本：

$$
\begin{align*}
    \softplus(z) = \log (1 + \exp(z))
\end{align*}
$$

其导数为 Logistic 函数

$$
\begin{align*}
    \softplus'(z) = \frac{\exp(z)}{1 + \exp(z)} = \frac{1}{1 + \exp(-z)}
\end{align*}
$$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER ReLU 族

<img src="../python/ReLU.svg" class="width60 center top6">

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="自门控的意思是控制自己是否激活的\sigma (\beta z)也跟有关" -->

HEADER Swish 函数

Swish 函数是一种自门控 (self-gated) 激活函数：

$$
\begin{align*}
    \swish(z) = z \cdot \sigma (\beta z) = \frac{z}{1 + \exp(-\beta z)}
\end{align*}
$$

其中$\beta$是可学习的参数或一个固定超参数

- 当$\sigma (\beta z)$接近于$1$时，门处于<span class="blue">开</span>状态，激活函数的输出近似于$z$本身
- 当$\sigma (\beta z)$接近于$0$时，门处于<span class="blue">关</span>状态，激活函数的输出近似于$0$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER Swish 函数

<img src="../python/Swish.svg" class="width60 center top6">

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER Maxout 单元

考虑神经网络的第$l$层：

$$
\begin{align*}
    \zv_l & = \Wv_l \av_{l-1} + \bv_l \\
    \av_l & = h_l (\zv_l)
\end{align*}
$$

前面提到的激活函数都是$\Rbb \mapsto \Rbb$的，即$[\av_l]_i = h_l ([\zv_l]_i), ~ i \in [n_l]$

<br>

Maxout 单元是$\Rbb^{n_l} \mapsto \Rbb$的，输入就是$\zv_l$，其定义为

$$
\begin{align*}
    \maxout (\zv) = \max_{k \in [K]} \{ \wv_k^\top \zv + b_k \}
\end{align*}
$$

- 整体学习输入到输出间的非线性关系
- $\relu(z) = \max \{ 0, z \}$与$\lrelu(z) \overset{\gamma < 1}{=} \max \{ z, \gamma z \}$都是 Maxout 单元的特例

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 应用到机器学习

@import "../dot/ml-nn.dot"

<div></div>

前$L-1$层是复合函数$\psi: \Rbb^d \mapsto \Rbb^{n_{L-1}}$，可以看作一种特征变换方法

<br>

最后一层是学习器$\hat{\yv} = g(\psi(\xv); \Wv_L, \bv_L)$，对输入$\psi(\xv)$进行预测

- 若$y \in \{ \pm 1 \}$，最后一层只需$1$个神经元，并采用 Logistic 激活函数
- 若$y \in [C]$，最后一层需$C$个神经元，并采用 Softmax 激活函数

<br>

因此对数几率回归也可看作只有一层 (没有隐藏层) 的神经网络

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 深度学习

传统机器学习：特征处理和学习两阶段分开进行

@import "../dot/ml-old.dot"

<br>

深度学习：特征工程和模型学习合二为一，端到端 (end-to-end)

@import "../dot/ml-nn.dot"

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 用 tensorflow 实现

```python {.line-numbers}
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import SGD

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

model = Sequential()
model.add(Dense(64, activation="relu"))
model.add(Dense(64, activation="relu"))
model.add(Dense(1, activation="sigmoid"))
model.compile(optimizer=SGD(0.001),
              loss="binary_crossentropy",
              metrics=["accuracy"],
              )

model.fit(X_train, y_train, epochs=10)
model.evaluate(X_test, y_test, verbose=2)

Epoch 1/10
14/14 [================] - 1s 1ms/step - loss: 51.0188 - accuracy: 0.4906
Epoch 2/10
14/14 [================] - 0s 1ms/step - loss: 1.0154 - accuracy: 0.7465
Epoch 3/10
14/14 [================] - 0s 1ms/step - loss: 0.5027 - accuracy: 0.8146
Epoch 4/10
14/14 [================] - 0s 1ms/step - loss: 0.4219 - accuracy: 0.8239
Epoch 5/10
14/14 [================] - 0s 1ms/step - loss: 0.4142 - accuracy: 0.8380
Epoch 6/10
14/14 [================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.8779
Epoch 7/10
14/14 [================] - 0s 1ms/step - loss: 0.2744 - accuracy: 0.8944
Epoch 8/10
14/14 [================] - 0s 1ms/step - loss: 0.2454 - accuracy: 0.9061
Epoch 9/10
14/14 [================] - 0s 1ms/step - loss: 0.3001 - accuracy: 0.8897
Epoch 10/10
14/14 [================] - 0s 1ms/step - loss: 0.2557 - accuracy: 0.8991

5/5 - 0s - loss: 0.2264 - accuracy: 0.9231
```

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 求解参数

设采用交叉熵损失，对样本$(\xv, y)$，损失函数为$\Lcal (\yv, \hat{\yv}) = - \yv \log \hat{\yv}$

<br>

优化目标为

$$
\begin{align*}
    \min_{\Wv, \bv} ~ \frac{1}{2} \| \Wv \|_F^2 + \frac{\lambda}{m} \sum_{i \in [m]} \Lcal (\yv_i, \hat{\yv}_i)
\end{align*}
$$

<br>

梯度下降 (标量对某矩阵求导的结果的尺寸与该矩阵呈转置关系)

$$
\begin{align*}
    \Wv & ~ \leftarrow ~ \Wv - \eta \left( \frac{\lambda}{m} \sum_{i \in [m]} \class{yellow}{\frac{\partial \Lcal (\yv_i, \hat{\yv}_i)}{\partial \Wv^\top}} + \Wv \right) \\
    \bv & ~ \leftarrow ~ \bv - \eta \cdot \frac{\lambda}{m} \sum_{i \in [m]} \class{yellow}{\frac{\partial \Lcal (\yv_i, \hat{\yv}_i)}{\partial \bv^\top}}
\end{align*}
$$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 求解参数

整个网络：$\xv = \av_0 \xrightarrow{\Wv_1,\bv_1} \zv_1 \xrightarrow{h_1} \av_1 \xrightarrow{\Wv_2,\bv_2} \cdots \xrightarrow{\Wv_L,\bv_L} \zv_L \xrightarrow{h_L} \av_L = \hat{\yv}$

<br>

损失$\Lcal (\yv, \hat{\yv})$的计算为<span class="blue">正向传播</span>

- 样本从输入层进入，经隐藏层逐层传播到最后输出层
- $\hat{\yv} = \av_L = h_L (\zv_L)$是对样本$\xv$的预测，据此计算$\Lcal (\yv, \hat{\yv}) = \Lcal (\yv, h_L (\zv_L))$

<br>

先看最后一层$\zv_L = \Wv_L ~ \av_{L-1} + \bv_L$，$\av_L = h_L (\zv_L)$，由<span class="blue">链式法则</span> (?) 有

$$
\begin{align*}
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \bv_L} & = \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \zv_L} \frac{\partial \zv_L}{\partial \bv_L} = \deltav_L^\top \frac{\partial \zv_L}{\partial \bv_L} \\
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \Wv_L} & = \sum_{j \in [n_L]} \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial [\zv_L]_j} \frac{\partial [\zv_L]_j}{\partial \Wv_L} = \sum_{j \in [n_L]} [\deltav_L]_j \frac{\partial [\zv_L]_j}{\partial \Wv_L}
\end{align*}
$$

其中$\deltav_L^\top = \partial \Lcal (\yv, \hat{\yv}) / \partial \zv_L \in \Rbb^{n_L}$为第$L$层的<span class="blue">误差项</span>，该项可直接求解

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 求解参数 反向传播

整个网络：$\xv = \av_0 \xrightarrow{\Wv_1,\bv_1} \zv_1 \xrightarrow{h_1} \av_1 \xrightarrow{\Wv_2,\bv_2} \cdots \xrightarrow{\Wv_L,\bv_L} \zv_L \xrightarrow{h_L} \av_L = \hat{\yv}$

<br>

类似的对第$l$层$\zv_l = \Wv_l \av_{l-1} + \bv_l$，$\av_l = h_l (\zv_l)$，由<span class="blue">链式法则</span> (?) 有

$$
\begin{align*}
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \bv_l} = \deltav_l^\top \frac{\partial \zv_l}{\partial \bv_l}, \quad \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \Wv_l} = \sum_{j \in [n_l]} [\deltav_l]_j \frac{\partial [\zv_l]_j}{\partial \Wv_l}
\end{align*}
$$

其中$\deltav_l^\top = \partial \Lcal (\yv, \hat{\yv}) / \partial \zv_l \in \Rbb^{n_l}$为第$l$层的<span class="blue">误差项</span>

<br>

误差<span class="blue">反向传播</span> (**b**ack**p**ropagation, BP)：前一层的误差可由后一层得到

$$
\begin{align*}
    \deltav_{l-1}^\top = \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \zv_{l-1}} = \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \zv_l} \frac{\partial \zv_l}{\partial \av_{l-1}} \frac{\partial \av_{l-1}}{\partial \zv_{l-1}} = \deltav_l^\top \frac{\partial \zv_l}{\partial \av_{l-1}} \frac{\partial h_{l-1}(\zv_{l-1})}{\partial \zv_{l-1}}
\end{align*}
$$

对第$l$层$\zv_l = \Wv_l \av_{l-1} + \bv_l$，如何求$\partial \zv_l / \partial \av_{l-1}$、$\partial \zv_l / \partial \bv_l$、$\partial [\zv_l]_j / \partial \Wv_l$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 求解参数 反向传播

对$\zv = \Wv \av + \bv$，如何求$\partial \zv / \partial \av$、$\partial \zv / \partial \bv$、$\partial z_j / \partial \Wv$

由矩阵求导公式易知

$$
\begin{align*}
    \frac{\partial \zv}{\partial \av} = \frac{\partial (\Wv \av)}{\partial \av} = \Wv, \quad \frac{\partial \zv}{\partial \bv} = \frac{\partial \bv}{\partial \bv} = \Iv
\end{align*}
$$

注意$z_j = \sum_k w_{jk} a_k + b_k$只与$\Wv$的第$j$行有关，于是

$$
\begin{align*}
    \frac{\partial z_j}{\partial \Wv} = \underbrace{\begin{bmatrix} \zerov, \ldots, \av, \ldots, \zerov \end{bmatrix}}_{\text{only }\av\text{ at }j\text{-th column}} = \av \ev_j^\top
\end{align*}
$$

从而

$$
\begin{align*}
    \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \Wv_l} = \sum_{j \in [n_l]} [\deltav_l]_j \frac{\partial [\zv_l]_j}{\partial \Wv_l} = \av_{l-1} \sum_{j \in [n_l]} [\deltav_l]_j \ev_j^\top = \av_{l-1} \deltav_l^\top
\end{align*}
$$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 反向传播算法

输入：训练集$\Scal$，验证集$\Vcal$，以及相关超参数

1. 随机初始化$\Wv$和$\bv$
2. repeat
3. &emsp;&emsp;对训练集$\Scal$中的样本随机重排序
4. &emsp;&emsp;for $i = 1, \ldots, m$ do
5. &emsp;&emsp;&emsp;&emsp;获取样本$(\xv_i, y_i)$
6. &emsp;&emsp;&emsp;&emsp;前向传播，计算每一层的$\zv_l = \Wv_l \av_{l-1} + \bv_l$，直到最后一层
7. &emsp;&emsp;&emsp;&emsp;反向传播计算每一层的误差项$\deltav_l^\top = \deltav_{l+1}^\top \Wv_{l+1} \diag (h_l'(\zv_l))$
8. &emsp;&emsp;&emsp;&emsp;计算梯度$\partial \Lcal / \partial \Wv_l = \av_{l-1} \deltav_l^\top$、$\partial \Lcal / \partial \bv_l = \deltav_l^\top$
9. &emsp;&emsp;&emsp;&emsp;采用梯度下降更新$\Wv_l$和$\bv_l$
10. &emsp;&emsp;end
11. until 神经网络模型在验证集$\Vcal$上的错误率不再下降

输出：$\Wv$和$\bv$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 梯度消失

神经网络中误差反向传播的迭代公式为

$$
\begin{align*}
    \deltav_l^\top = \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \zv_l} = \frac{\partial \Lcal (\yv, \hat{\yv})}{\partial \zv_{l+1}} \frac{\partial \zv_{l+1}}{\partial \av_l} \frac{\partial \av_l}{\partial \zv_l} = \deltav_{l+1}^\top \Wv_{l+1} \diag (h_l'(\zv_l))
\end{align*}
$$

<br>

对于 Sigmoid 型激活函数

- $\sigma'(z) = \sigma(z) (1 - \sigma(z)) \leq \frac{1}{4}$
- $\tanh'(z) = 4 \sigma(2z) (1 - \sigma(2z)) \leq 4 \cdot \frac{1}{4} = 1$

<br>

误差每传播一层都会乘以一个小于等于$1$的系数，当网络层数很深时，梯度会不断衰减甚至消失，使得整个网络很难训练

<br>

方案：使用导数比较大的激活函数，比如 ReLU

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 残差网络

<img src="../tikz/resnet.svg" class="width75 center top2 bottom2">

残差模块：$\zv_l = \av_{l-1} + \class{yellow}{\Uv_2 \cdot h(\Uv_1 \cdot \av_{l-1} + \cv_1) + \cv_2} = \av_{l-1} + \class{yellow}{f(\av_{l-1})}$

假设$\av_l = \zv_l$，即残差模块输出不使用激活函数，对$\forall t \in [l]$有

$$
\begin{align*}
    \av_l & = \av_{l-1} + f(\av_{l-1}) = \av_{l-2} + f(\av_{l-2}) + f(\av_{l-1}) \\
    & = \cdots = \av_{l-t} + \sum_{i=l-t}^{l-1} f(\av_i)
\end{align*}
$$

低层输入可以<span class="blue">恒等</span>传播到任意高层

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 残差网络

$$
\begin{align*}
    \av_l = \av_{l-t} + \sum_{i=l-t}^{l-1} f(\av_i)
\end{align*}
$$

由链式法则有

$$
\begin{align*}
    \frac{\partial \Lcal}{\partial \av_{l-t}} & = \frac{\partial \Lcal}{\partial \av_l} \frac{\partial \av_l}{\partial \av_{l-t}} = \frac{\partial \Lcal}{\partial \av_l} \left( \frac{\partial \av_{l-t}}{\partial \av_{l-t}} + \frac{\partial }{\partial \av_{l-t}} \sum_{i=l-t}^{l-1} f(\av_i) \right) \\
    & = \frac{\partial \Lcal}{\partial \av_l} \left( \Iv + \frac{\partial }{\partial \av_{l-t}} \sum_{i=l-t}^{l-1} f(\av_i) \right) \\
    & = \frac{\partial \Lcal}{\partial \av_l} + \frac{\partial \Lcal}{\partial \av_l} \left( \frac{\partial }{\partial \av_{l-t}} \sum_{i=l-t}^{l-1} f(\av_i) \right)
\end{align*}
$$

高层误差可以<span class="blue">恒等</span>传播到任意低层，梯度消失得以缓解

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 优化算法

神经网络通常采用小批量梯度下降：

$$
\begin{align*}
    \Wv^\top ~ \leftarrow ~ \Wv^\top - \frac{\eta}{|\Bcal|} \sum_{i \in \Bcal} \frac{\partial \Lcal (\yv_i, \hat{\yv}_i)}{\partial \Wv}
\end{align*}
$$

批量大小$|\Bcal|$不影响随机梯度的期望，但会影响方差

- $|\Bcal|$越大，方差越小，训练越稳定，可以采用较大的步长加快收敛
- $|\Bcal|$越小，方差越大，需采用较小的步长，否则可能不收敛

<br>

线性缩放规则：$|\Bcal|$增加$k$倍，步长也增加$k$倍，但当$|\Bcal|$特别大时，线性缩放也还是会出现训练不稳定

- 步长调整
- 更新方向调整

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 步长调整

@import "../dot/nn-stepsize.dot"

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 步长衰减

基本想法：在一开始要使用大步长来保证收敛速度，在接近最优解时要用小步长避免来回震荡，也称为步长退火

<br>

设初始步长为$\eta_0$，第$t$次迭代时的步长为$\eta_t$，常见的衰减方式为根据迭代次数进行衰减

- 分段常数衰减：每经过$T_1, T_2, \ldots, T_n$次迭代步长衰减为原来的$\beta_1, \beta_2, \ldots, \beta_n$倍，其中$T_n$和$\beta_n < 1$为超参数
- 逆时衰减：$\eta_t = \eta_0 / (1 + \beta * t)$，其中$\beta$为衰减率
- 指数衰减：$\eta_t = \eta_0 \beta^t$，其中$\beta < 1$为衰减率
- 自然指数衰减：$\eta_t = \eta_0 \exp(-\beta * t)$
- 余弦衰减：$\eta_t = \eta_0 (1 + \cos (t \pi /T) )$，其中$T$为总的迭代次数

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 步长预热

在训练初始阶段，由于参数是随机初始化的，梯度往往也比较大，如果初始步长也很大，会使得训练不稳定

<br>

步长预热：

- 在最初几轮迭代时，采用较小的步长
- 等梯度下降到一定程度后再恢复为初始步长

<br>

假设预热迭代次数为$T'$，初始步长为$\eta_0$，则

$$
\begin{align*}
    \eta'_t = \frac{t}{T'} \eta_0, \quad t \in [T']
\end{align*}
$$

当预热过程结束，再选择一种步长衰减方法来逐渐降低步长

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 周期性步长

在训练过程中周期性地增大步长

<br>

目的：在训练过程中有助于逃离<span class="blue">尖锐的局部最小值点</span>和<span class="blue">鞍点</span>

- 平坦的局部最小值点：鲁棒性好，微小的参数变动不会对模型有剧烈影响
- 尖锐的局部最小值点：鲁棒性差，微小的参数变动也会导致模型剧烈变化

<br>

周期性地增大步长虽然会短期内会损害优化过程，但通常最终会收敛到更加理想的局部极小值点，类似于模拟退火

- <span class="blue">循环步长</span>：每个循环周期的长度都为$2 \Delta T$，前$\Delta T$轮步长线性增大，后$\Delta T$轮步长线性缩小，第$n$个周期中步长的上界和下界随着$m$的增大而逐渐减小
- <span class="blue">带热重启的 SGD</span>：步长每隔一定周期后重新初始化为某个预先设定的值，然后逐渐衰减；每次重启后模型参数不是从头开始优化，而是在重启前的参数基础上继续优化

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 自适应步长

AdaGrad：第$t$轮迭代先计算每个参数<span class="blue">梯度平方的累积值</span>：

$$
\begin{align*}
    \Gv_t = \sum_{\tau \in [t]} \gv_\tau \odot \gv_\tau
\end{align*}
$$

其中$\odot$为按元素乘积，$\gv_\tau \in \Rbb^{|\Wv|}$是第$\tau$次迭代时的梯度

<br>

再利用累积梯度平方做衰减 (每个元素各自计算)

$$
\begin{align*}
    \Wv_{t+1} \leftarrow \Wv_t - \frac{\alpha}{\sqrt{\Gv_t + \epsilon}} \odot \gv_\tau
\end{align*}
$$

其中$\alpha$是初始步长，$\epsilon$是为了保持数值稳定而设置的非常小的常数

<br>

缺点：经过一定轮数的迭代仍未找到最优点时，由于此时步长已经非常小，很难再继续找到最优点

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 自适应步长

RMSprop：第$t$轮迭代先计算<span class="blue">梯度平方的指数衰减移动平均</span>：

$$
\begin{align*}
    \Gv_t = \beta \Gv_{t-1} + (1 - \beta) \gv_\tau \odot \gv_\tau = (1 - \beta) \sum_{\tau \in [t]} \beta^{t - \tau} \gv_\tau \odot \gv_\tau
\end{align*}
$$

其中$\beta < 1$为衰减率，一般取值$0.9$

<br>

RMSprop 的更新公式为

$$
\begin{align*}
    \Wv_{t+1} \leftarrow \Wv_t - \frac{\alpha}{\sqrt{\Gv_t + \epsilon}} \odot \gv_\tau
\end{align*}
$$

其中$\alpha$通常设为$0.001$

<br>

较 AdaGrad 的优点：$\Gv_t$并非单调增加，故步长不是单调衰减，既可以变小也可以变大

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 自适应步长

AdaDelta：进一步引入<span class="blue">参数更新差平方的指数衰减移动平均</span>：

$$
\begin{align*}
    \Delta \Uv_{t-1}^2 = \beta \Delta \Uv_{t-2}^2 + (1 - \beta) \Delta \Wv_{t-1} \odot \Delta \Wv_{t-1}
\end{align*}
$$

其中$\beta$为衰减率，$\Delta \Wv_{t-1} = \Wv_t - \Wv_{t-1}$为参数更新差

<div class="bottom4"></div>

AdaDelta 的更新公式为

$$
\begin{align*}
    \Wv_{t+1} \leftarrow \Wv_t - \frac{\sqrt{\Delta \Uv_{t-1}^2 + \epsilon}}{\sqrt{\Gv_t + \epsilon}} \odot \gv_\tau
\end{align*}
$$

优点：将 RMSprop 中的初始步长$\alpha$改为动态计算的$\Delta \Uv_{t-1}$，在一定程度上抑制了步长的波动

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 更新方向调整

动量法：计算负梯度的“加权移动平均”作为参数的更新方向

$$
\begin{align*}
    \Wv_{t+1} - \Wv_t = \Delta \Wv_t = \rho \Delta \Wv_{t-1} - \alpha \gv_t = - \alpha \sum_{\tau \in [t]} \rho^{t - \tau} \gv_\tau
\end{align*}
$$

Nesterov 加速梯度：改进动量法的第二步

$$
\begin{align*}
    \begin{cases} \widetilde{\Wv} = \Wv_t + \rho \Delta \Wv_{t-1} \\ \Wv_{t+1} = \widetilde{\Wv} - \alpha ~ \class{yellow}{\gv_t (\Wv_t)} \end{cases}
    ~ \Longrightarrow ~
    \begin{cases} \widetilde{\Wv} = \Wv_t + \rho \Delta \Wv_{t-1} \\ \Wv_{t+1} = \widetilde{\Wv} - \alpha ~ \class{yellow}{\gv_t (\widetilde{\Wv})} \end{cases}
\end{align*}
$$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 更新方向调整

Adam (**ada**ptive **m**oment estimation)：动量法和 RMSprop 的结合

$$
\begin{align*}
    \Mv_t & = \beta_1 \Mv_{t-1} + (1 - \beta_1) \gv_t = (1 - \beta_1) \sum_{\tau \in [t]} \beta_1^{t - \tau} \gv_\tau \\
    \Gv_t & = \beta_2 \Gv_{t-1} + (1 - \beta_2) \gv_t \odot \gv_t = (1 - \beta_2) \sum_{\tau \in [t]} \beta_2^{t - \tau} \gv_\tau \odot \gv_\tau
\end{align*}
$$

其中$\beta_1$、$\beta_2$为衰减率，一般取值$\beta_1 = 0.9$、$\beta_2 = 0.99$

<br>

$$
\begin{align*}
    \Ebb [\Mv_t] & = (1 - \beta_1) \sum_{\tau \in [t]} \beta_1^{t - \tau} \Ebb [\gv_\tau] = (1 - \beta_1^t) \Ebb [\gv_\tau] \\
    \Ebb [\Gv_t] & = (1 - \beta_2) \sum_{\tau \in [t]} \beta_2^{t - \tau} \Ebb [\gv_\tau \odot \gv_\tau] = (1 - \beta_2^t) \Ebb [\gv_\tau \odot \gv_\tau]
\end{align*}
$$

因此$\widetilde{\Mv}_t = \Mv_t / (1 - \beta_1^t)$可以看作一阶矩，$\widetilde{\Gv}_t = \Gv_t / (1 - \beta_2^t)$为二阶矩

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 更新方向调整

Adam 的更新公式为

$$
\begin{align*}
    \Wv_{t+1} \leftarrow \Wv_t - \frac{\alpha}{\sqrt{\widetilde{\Gv}_t + \epsilon}} \odot \widetilde{\Mv}_t
\end{align*}
$$

其中$\alpha$通常设为$0.001$，也可以进行衰减，例如$\alpha_t = \alpha / \sqrt{t}$

<br>

如果将 NAG 和 RMSprop 的结合，则得到 Nadam

<br>

对于深层网络，在基于梯度下降的训练过程中，除了梯度消失，也会出现梯度爆炸，此时可进行梯度截断

- 按值截断：$\gv_t = \max \{ \min \{ \gv_t, b \}, a \}$
- 按范数截断：$\gv_t = b ~ \gv_t / \| \gv_t \|$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 训练技巧

训练神经网络有很多奇技淫巧

- 参数初始化
- 逐层归一化
- 超参数选择
- 权重衰减
- 提前停止
- 随机丢弃
- 数据增强
- Mixup

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 参数初始化

感知机、支持向量机、对数几率回归的$\wv$通常初始化为零

<br>

神经网络的$\Wv$如果全部初始化为零，在第一遍前向计算时，所有的隐层神经元的激活值都相同，这样会导致深层神经元没有区分性

<br>

方案：随机初始化

<br>

策略：<span class="blue">保持每个神经元输入和输出的方差一致</span>

<br>

第$l$个隐层的神经元$z$接受前一层的输出$a_1, \ldots, a_{n_{l-1}}$作为输入

$$
\begin{align*}
    z = \sum_{i \in [n_{l-1}]} w_i a_i
\end{align*}
$$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 参数初始化

假设$w_i$和$a_i$的均值都为$0$，并且互相独立，则$z$的方差为

$$
\begin{align*}
    \var[z] = \sum_{i \in [n_{l-1}]} \var[w_i] \var[a_i] = n_{l-1} \var[w_i] \var[a_i]
\end{align*}
$$

<br>

若想保持每个神经元的输入和输出的方差一致，则有$\var[w_i] = 1 / n_{l-1}$

<br>

同理在反向传播中，若想误差信号也不被放大或缩小，需将$w_i$的方差保持为$\var[w_i] = 1 / n_l$

<br>

两相折中，可以设置$\var[w_i] = 2 / (n_l + n_{l-1})$

- <span class="blue">正态分布初始化</span>，$\Ncal (0, \sqrt{2 / (n_l + n_{l-1})})$
- <span class="blue">均匀分布初始化</span>，若分布区间为$[-r,r]$，则$r = \sqrt{6 / (n_l + n_{l-1})}$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 逐层归一化

在使用随机梯度下降来训练网络时

- 每次参数更新都会导致网络中间每一层的输入的分布发生改变
- 越深的层的输入分布会改变得越明显

如果某个神经层的输入分布发生了改变，那么其参数需要重新学习

<span class="blue">批量归一化</span> (**b**atch **n**ormalization, BN)：逐层将各个神经元$z$归一化到标准正态分布

$$
\begin{align*}
    \hat{z} = \frac{z - \Ebb[z]}{\sqrt{\var [z] + \epsilon}}
\end{align*}
$$

$z$的期望和方差通常用当前小批量样本集的均值和方差近似估计

批量归一化操作可以看作一个特殊的层，加在每一层非线性激活函数前：$\av_{l+1} = h(\mathrm{BN} (\Wv \av_l))$

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 逐层归一化

批量归一化：针对单个神经元

- 要求小批量样本数不能太小，否则难以得到单个神经元较准确的统计信息

层归一化：针对一层的所有神经元

<br>

设小批量样本数为$k$，该层神经元数为$n$

$$
\begin{align*}
    \begin{bmatrix}
        z_{11} & z_{12} & \cdots & z_{1n} \\
        z_{21} & z_{22} & \cdots & z_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        z_{k1} & z_{k2} & \cdots & z_{kn} \\
    \end{bmatrix}
\end{align*}
$$

<br>

- 批量归一化：对列做归一化
- 层归一化：对行做归一化，用于小批量样本数较小的时候

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 超参数选择

超参数对神经网络的性能影响很大，常见的超参数有

- 网络结构：神经元之间的连接关系、层数、每层的神经元数量、激活函数类型
- 优化参数：优化方法、步长、小批量样本数
- 正则化系数

<br>

超参数优化很难

- 组合优化问题，无法像一般参数那样通过梯度下降方法来优化，也没有一种通用有效的优化方法
- 评估一组超参数配置的时间代价非常高，从而导致一些黑盒优化方法，如演化算法难以应用

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 网格搜索

尝试所有的超参数组合来寻找一组合适的超参数配置

<br>

设共有$K$个超参数，第$k$个超参数可以取$n_k$个值，那么组合总数为

$$
\begin{align*}
    n_1 \times n_2 \times \cdots \times n_K
\end{align*}
$$

<br>

如果超参数是连续的，可以将超参数离散化，选择几个“经验”值，比如正则化系数$\lambda$，可以设置

$$
\begin{align*}
    \lambda \in \{ 0.01, 0.1, 1, 10, 100 \}
\end{align*}
$$

<br>

对于连续的超参数，不能简单地按等间隔的方式离散化，需要根据超参数自身的特点进行离散化

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 随机搜索

不同超参数对模型性能的影响有很大差异

<br>

有些超参数对模型性能的影响有限，例如正则化系数；而有些超参数对模型性能影响比较大，例如步长

<br>

<span class="blue">采用网格搜索会在不重要的超参数上进行不必要的尝试</span>

<br>

随机搜索：对超参数进行随机组合，然后选取一个性能最好的配置

<br>

优点：在实践中更容易实现，一般会比网格搜索更加有效

<br>

缺点：与网格搜索一样，没有利用不同超参数组合之间的相关性，即如果超参数组合比较类似，模型性能也会比较接近，因此这两种搜索方式一般都比较低效

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 贝叶斯优化

根据已试验的超参数组合，猜测可能带来更大收益的组合

- 如何根据已有超参数组合对应的模型性能，得到未知组合的模型性能
- 如何确定收益

<br>

第一个问题通常采用<span class="blue">高斯过程回归</span>，此时$p(\mathtip{g}{泛化风险}|\mathtip{h}{超参数})$为一个正态分布

$$
\begin{align*}
    \Scal = \{ g_i, h_i \}_{i \in [m]} ~ \Longrightarrow ~ p(\hat{g} | \hat{h}, \Scal)
\end{align*}
$$

<br>

第二个问题需引入一个收益函数，常见的是期望改善

$$
\begin{align*}
    \int \max \{ g^\star - g, 0 \} p(g | h, \Scal) \diff g
\end{align*}
$$

其中$g^\star = \min \{ g_i, i \in [m] \}$是当前已有超参数组合中的最优泛化风险

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 动态资源分配

在超参数优化中，如果可以在早期就估计出一个超参数组合的效果会比较差，那么可以提早停止对它的评估，将更多的计算资源留给其它更有潜力的超参数组合

<br>

逐次减半法：

- 将所有计算资源平均分给所有的超参数组合
- 同时训练每个超参数组合对应的网络一段时间
- 保留前一半好的组合，转第 1 步

<br>

利用 - 探索两难问题：

- 如果超参数组合数越多，得到最佳组合的可能性也越大，但每个组合分到的计算资源就越少，早期的评估结果可能不准
- 如果超参数组合数越少，每个超参数组合的评估会越准确，但有可能无法得到最优组合

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 神经架构搜索

深度学习：“特征工程”问题 → “网络架构工程”问题

<br>

神经架构搜索 (**n**eural **a**rchitecture **s**earch, NAS)：用神经网络来自动实现网络架构的设计，目前最热 (内) 门 (卷) 的研究方向

- 神经网络的架构可以用一个变长的字符串来描述
- 用另一个循环神经网络来不断生成新的架构描述
- 循环神经网络的训练采用强化学习来完成，奖励信号可以为生成的网络在验证集上的性能

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="避免网络过拟合" -->

HEADER 网络正则化

权重衰减：每次更新时引入一个衰减系数

$$
\begin{align*}
    \Wv_{t+1} & \leftarrow (1 - \beta) \Wv_t - \eta \gv_t = \Wv_t - \eta \left( \gv_t + \frac{\beta}{\eta} \Wv_t \right)
\end{align*}
$$

- 在标准的随机梯度下降中，权重衰减等价于$\ell_2$正则
- 在较为复杂的优化方法，例如 Adam 中，两者并不等价

<br>

提前停止：

- 引入一个和训练集独立的样本集合，称为验证集 (validation set)，验证集上的错误可视为期望错误
- 当验证集上的错误率不再下降，就停止训练

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER 随机丢弃

对每一个神经元都以固定的概率$p$判定是否要保留

@import "../dot/dropout.dot"

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

HEADER 数据增强

深层神经网络需要大量的训练数据才能获得比较理想的效果

<br>

数据量有限的情况下，可以通过数据增强来增加数据量，避免过拟合

<br>

目前数据增强主要用于图像数据，文本等其它类型的数据还没有太好的方法

<br>

常见的增强方法：

- 旋转：将图像随机旋转一定角度
- 翻转：将图像沿水平或垂直方法随机翻转一定角度
- 缩放：将图像放大或缩小一定比例
- 平移：将图像沿水平或垂直方法平移一定距离
- 加噪声：加入随机噪声

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

HEADER mixup

利用任意两个样本$(\xv_a, y_a)$、$(\xv_b, y_b)$生成新样本

$$
\begin{align*}
    (\beta \xv_a + (1 - \beta) \xv_b, \beta y_a + (1 - \beta) y_b)
\end{align*}
$$

<br>

假设模型已经有能力预测$y_a = f(\xv_a)$、$y_b = f(\xv_b)$，那么此时还需满足

$$
\begin{align*}
    f(\beta \xv_a + (1 - \beta) \xv_b) = \beta y_a + (1 - \beta) y_b = \beta f(\xv_a) + (1 - \beta) f(\xv_b)
\end{align*}
$$

<br>

这个函数方程的解是线性函数，即 mixup 希望学到的$f$是线性函数

<div class="bottom6"></div>

### 披着数据增强外衣的正则化方法！

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="灵材：可免费获取的 MNIST 有 10 类，ImageNet 则有上千类，丹师是从药童做起，多模态：混合灵草和妖兽 <br><br> 丹方里最重要的是灵阵，控制如何抽取和凝结灵材中的灵性。灵阵中有若干节点，然后通过回路连接这些节点。灵材沿着回路游走经过每个节点处进行一步一步的提纯 <br><br> 半自动 不用你手动求导 做反向传播 更高端的可以使用多个丹炉同时开火炼制一枚灵丹 tf boy pt boy <br><br> 手中富裕的买 囊中羞涩的租" -->

HEADER 总结 当代炼丹术

```dot
digraph g {
    graph [nodesep=0.2, ranksep=0.4]
    rankdir=LR
    node [shape=plaintext color="#586e75" fontname="EBG,fzlz" fontcolor="#b58900" fontsize=18]
    edge [arrowhead=vee color="#586e75" fontname="EBG,fzlz" fontcolor="#268bd2" fontsize=14 arrowsize=0.5]
    bgcolor="transparent"

    subgraph cluster_1 {
        color="#586e75"
        fontcolor="#586e75"
        style="dashed"
        fontname="EBG,fzlz"
        label="增强品质"

        灵材

        node [fontcolor="#268bd2"]

        空间属性 时间属性 图属性
    }

    subgraph cluster_2 {
        color="#586e75"
        fontcolor="#586e75"
        style="dashed"
        fontname="EBG,fzlz"
        label="设计灵阵"

        丹方

        node [fontcolor="#268bd2"]

        卷积类 循环类 图类
    }

    subgraph cluster_3 {
        color="#586e75"
        fontcolor="#586e75"
        style="dashed"
        fontname="EBG,fzlz"
        label="精通用法"

        丹炉

        node [fontcolor="#268bd2"]

        半自动炼制 TensorFlow PyTorch
    }

    subgraph cluster_4 {
        color="#586e75"
        fontcolor="#586e75"
        style="dashed"
        fontname="EBG,fzlz"
        label="氪金"

        真火

        node [fontcolor="#268bd2"]

        炼制速度 "售 核弹厂" "租 阿里云"
    }

    subgraph cluster_5 {
        color="#586e75"
        fontcolor="#586e75"
        style="dashed"
        fontname="EBG,fzlz"
        label="控制调节"

        炼制

        node [fontcolor="#268bd2"]

        批量大小 随机丢弃 提早停止
    }

    灵材 -> 丹方 -> 丹炉 -> 真火 -> 炼制
}
```

一个优秀丹师的自我修养：

- 灵材品质差要会手动增强，旋转、翻转、缩放、平移、加噪声、标记平滑
- 因材制宜设计灵阵，空间属性灵材用卷积类灵阵，时间属性灵材用循环类...
- 仔细观察丹炉状态，防止爆炉，若最终仙丹成色不好则改进配置重新来过

FOOTER3 图神经网络导论 神经网络 tengzhang@hust.edu.cn
