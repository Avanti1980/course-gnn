---
presentation:
  transition: "none"
  enableSpeakerNotes: true
  margin: 0
---

@import "../common/css/font-awesome-4.7.0/css/font-awesome.css"
@import "../common/css/zhangt-solarized.css"
@import "css/GNN.css"

<!-- slide data-notes="" -->

GNN-HEADER 对数几率回归

正则化项 + 损失函数：

$$
\begin{align*}
    \min_\wv ~ \lambda \cdot \Omega(\wv) + \frac{1}{m} \sum_{i \in [m]} l(y_i, f(\xv_i))
\end{align*}
$$

- 线性模型：$f(\xv) = \wv^\top \xv + b$
- 对数几率损失：$l(y, f(\xv)) = \log (1 + \exp (- y f(\xv)))$
- 正则化项：$\ell_2$正则

</br>

$$
\begin{align*}
    \min_{\wv,b} ~ \frac{1}{2} \| \wv \|_2^2 + \frac{\lambda}{m} \sum_{i \in [m]} \log (1 + \exp (- y_i (\wv^\top \xv_i + b)))
\end{align*}
$$

初衷：

- 让分类器的输出结果具有自然的<span class="blue">概率</span>意义
- 支持向量机的输出若想具有概率意义，还需进行额外的后处理

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 对数几率回归

引入单调递增的 Sigmoid 函数$\sigma: \Rbb \mapsto [0,1]$

$$
\begin{align*}
    \sigma(z) = \frac{1}{1 + \exp (-z)} = \begin{cases}
        1 & 若~z \rightarrow \infty \\
        0 & 若~z \rightarrow -\infty
    \end{cases}
\end{align*}
$$

则$P(y = 1 | \xv) \triangleq \sigma(\wv^\top \xv + b)$为预测为正类的概率，此时需度量预测结果分布$\pv = [\sigma(\wv^\top \xv + b); 1 - \sigma(\wv^\top \xv + b)]$与类别标记独热编码$\qv = [\frac{1+y}{2}; \frac{1-y}{2}]$之间的差别，刻画分布间差别最常用的就是<span class="blue">交叉熵</span>($\pv = \qv$时取最小)：

$$
\begin{align*}
    H & (\qv, \pv) = - \sum_i q_i \log p_i = \sum_i q_i \log (1/p_i) \\
    & = \frac{1+y}{2} \log (1 + \exp (-\wv^\top \xv - b)) + \frac{1-y}{2} \log (1 + \exp (\wv^\top \xv + b)) \\
    & = \begin{cases}
        \log (1 + \exp (-\wv^\top \xv - b)) & 若~ y = 1 \\
        \log (1 + \exp (\wv^\top \xv + b)) & 若~ y = -1
    \end{cases}
\end{align*}
$$

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 对数几率回归

$$
\begin{align*}
    H (\qv, \pv) & = \begin{cases}
        \log (1 + \exp (-\wv^\top \xv - b)) & 若~ y = 1 \\
        \log (1 + \exp (\wv^\top \xv + b)) & 若~ y = -1
    \end{cases} \\
    & = \log (1 + \exp (- y (\wv^\top \xv + b)))
\end{align*}
$$

</br>

交叉熵损失应用到两类问题上就退化成了对数几率损失

$$
\begin{align*}
    P_+ = \frac{1}{1 + \exp (-(\wv^\top \xv + b))} \Longrightarrow \wv^\top \xv + b = \ln \frac{P_+}{1-P_+}
\end{align*}
$$

- $P_+$是预测为正类的概率，$1-P_+$是预测为负类的概率
- 两者的比值反映了预测为正类的相对可能性，称为“<span class="blue">几率</span>”
- 用线性模型拟合几率的对数，故称为对数几率回归
- 虽然名为回归，但本质上是一个线性分类模型
- 让预测结果具有概率意义的代价是牺牲了解的稀疏性，预测开销更大

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER 多分类对数几率回归

对数几率回归可很自然地推广到多分类

</br>

预测函数$f(\xv) = \argmax_{c \in [C]} (\wv_c^\top \xv + b_c)$，第$c$类概率由 Softmax 函数有：

$$
\begin{align*}
    P(y = c | \xv) & = \frac{\exp (\wv_c^\top \xv + b_c)}{\sum_{c' \in [C]} \exp (\wv_{c'}^\top \xv + b_{c'})} \\
    & = \frac{\exp ((\wv_c - \wv_C)^\top \xv + b_c - b_C)}{\sum_{c' \in [C-1]} \exp ((\wv_{c'} - \wv_C)^\top \xv + b_{c'} - b_C) + 1}
\end{align*}
$$

令$\wv_c \leftarrow \wv_c - \wv_C$，$b_c \leftarrow b_c - b_C$，消去冗余的$\wv_C$、$b_C$可得

$$
\begin{align*}
    P(y = c | \xv) = \frac{\exp (\wv_c^\top \xv + b_c)}{\sum_{c' \in [C-1]} \exp (\wv_{c'}^\top \xv + b_{c'}) + 1}
\end{align*}
$$

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 多分类对数几率回归

对于样本$(\xv_i, y_i)$

$$
\begin{align*}
    & \pv_i = \frac{[ \exp (\wv_1^\top \xv_i + b_1), \ldots, \exp (\wv_{C-1}^\top \xv_i + b_{C-1}), 1 ]}{\sum_{c' \in [C-1]} \exp (\wv_{c'}^\top \xv_i + b_{c'}) + 1} \\
    & \qv_i = [1_{y_i=1}, 1_{y_i=2}, \ldots, 1_{y_i=C}] \\
    & H (\qv_i, \pv_i) = - \sum_{c \in [C]} [\qv_i]_c \log [\pv_i]_c
\end{align*}
$$

</br>

多分类对数几率回归的优化问题为

$$
\begin{align*}
    \min_{\wv_c, b_c} & ~ \frac{\lambda}{m} \sum_{i \in [m]} \sum_{c \in [C]} [\qv_i]_c \log \frac{1}{[\pv_i]_c} + \frac{1}{2} \sum_{c \in [C-1]} \| \wv_c \|_2^2
\end{align*}
$$

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 多分类对数几率回归

当$C = 2$时

$$
\begin{align*}
    & \quad ~ H(\qv, \pv) \\
    & = - 1_{y=1} \log \frac{\exp (\wv_1^\top \xv + b_1)}{ \exp (\wv_1^\top \xv + b_1) + 1} - 1_{y=2} \log \frac{1}{ \exp (\wv_1^\top \xv + b_1) + 1} \\
    & = 1_{y=1} \log (1 + \exp (- \wv_1^\top \xv - b_1)) + 1_{y=2} \log (1 + \exp (\wv_1^\top \xv + b_1))
\end{align*}
$$

将第$2$类类别标记记为$-1$，则$H (\qv, \pv) = \log (1 + \exp (- y (\wv_1^\top \xv + b_1)))$

</br>

神经网络视角：

|对数几率回归|层数|激活函数|输出层节点数|
|:--:|:--:|:--:|:--:|


<table class="lr" style="width:70%;margin-left:15%">
    <thead>
        <tr>
            <th>对数几率回归</th>
            <th>层数</th>
            <th>激活函数</th>
            <th>输出层节点数</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>$y \in \{ 1, -1 \}$</td>
            <td>一层</td>
            <td>Logistic</td>
            <td>$1$</td>
        </tr>
        <tr>
            <td>$y \in [C]$</td>
            <td>一层</td>
            <td>Softmax</td>
            <td>$C$</td>
        </tr>
    </tbody>
</table>

GNN-FOOTER 图神经网络导论 机器学习 tengzhang@hust.edu.cn
