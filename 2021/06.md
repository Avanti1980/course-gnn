---
presentation:
  transition: "none"
  enableSpeakerNotes: true
  margin: 0
---

@import "../common/css/font-awesome-4.7.0/css/font-awesome.css"
@import "../common/css/zhangt-solarized.css"
@import "css/GNN.css"

<!-- slide data-notes="" -->
<div class="header"><img class="hust"></div>

<div class="bottom15"></div>

# 图神经网络导论

<hr class="width50">

## 循环神经网络

<div class="bottom5"></div>

### 计算机科学与技术学院 &nbsp; &nbsp; 张腾

<br>

#### tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 大纲

@import "../dot/outline-rnn.dot"

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER 循环神经网络

前向神经网络

- 信息单向传递
- 输出只依赖于当前输入
- 难以处理变长的时序数据 (视频、语音、文本)

<br>

循环神经网络：

- 带有<span class="blue">环路</span>，神经元既可以接收其它神经元的信息，也可以接收自身的信息
- 目的是让网络具有<span class="blue">短期记忆</span>能力，能处理时序数据

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 短期记忆

<span class="blue">延时神经网络</span>：在前向网络的<span class="blue">非输出层</span>都添加一个<span class="blue">延时器</span>，记录最近几次的输出，在第$t$个时刻，第$l + 1$层神经元和第$l$层神经元的最近$p$次输出相关

$$
\begin{align*}
    \av^{(l+1)}_t = h(\av^{(l)}_t, \av^{(l)}_{t-1}, \ldots, \av^{(l)}_{t-p})
\end{align*}
$$

延时神经网络在时间维度上共享权值，对序列输入来讲延时神经网络就相当于卷积神经网络

<br>

<span class="blue">有外部输入的非线性自回归模型</span>：在每个时刻$t$都有一个外部输入$\xv_t$，产生一个输出$\yv_t$，延时器记录最近几次的外部输入和输出

$$
\begin{align*}
    \yv_t = h(\xv_t, \xv_{t-1}, \ldots, \xv_{t-p}, \yv_{t-1}, \ldots, \yv_{t-p})
\end{align*}
$$

其中$h$是一个非线性函数，可以是一个前向神经网络

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER 循环神经网络

给定输入序列$\xv_1, \xv_2, \ldots, \xv_t, \ldots, \xv_T$，循环神经网络的更新：

$$
\begin{align*}
    \av_t = h(\av_{t-1}, \xv_t)
\end{align*}
$$

其中$\av_0 = \zerov$，$h$是一个非线性函数，可以是一个前向神经网络

<img src="../tikz/rnn.svg" class="width30 left15 top4">

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 简单循环网络

只有一个隐藏层的循环神经网络，存在隐藏层到隐藏层的连边

$$
\begin{align*}
    \zv_t & = \class{yellow}{\Uv \av_{t-1}} + \Wv \xv_t + \bv \\
    \av_t & = h(\zv_t)
\end{align*}
$$

<br>

我的批注 循环神经网络可以看作是在时间维度上权值共享的神经网络

<img src="../tikz/rnn-simple.svg" style="margin-top:3%;" height=200px>

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER 动力系统观点

$$
\begin{align*}
    \zv_t & = \class{yellow}{\Uv \av_{t-1}} + \Wv \xv_t + \bv \\
    \av_t & = h(\zv_t)
\end{align*}
$$

循环神经网络的更新可以看成一个<span class="blue">动力系统</span> (dynamical system)，因此隐藏层的输出$\av_t$在很多文献上也称为<span class="blue">状态</span> (state)

<br>

动力系统：使用一个 (微分) 方程来描述给定空间中所有点随时间变化情况的系统

$$
\begin{align*}
    \wv_{t+1} = \wv_t - \eta f'(\wv_t) \Longrightarrow \frac{\wv_{t+1} - \wv_t}{\eta} = - f'(\wv_t) \Longrightarrow \dot{\wv} = - f'(\wv)
\end{align*}
$$

<br>

梯度下降就是在用 (前向) 欧拉法离散地求解动力系统

<br>

Nesterov 加速梯度的微分方程表示：$\ddot{\wv} + (3/t) \dot{\wv} = - f'(\wv)$

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 动力系统观点

梯度下降的微分方程表示：$\dot{\wv} = - f'(\wv)$

引入函数

$$
\begin{align*}
    \Ecal(t) = t (f(\wv) - f^\star) + \frac{1}{2} \| \wv - \wv^\star \|_2^2
\end{align*}
$$

易知

$$
\begin{align*}
    \Ecal'(t) & = f(\wv) - f^\star + t \dot{\wv}^\top f'(\wv) + \dot{\wv}^\top (\wv - \wv^\star) \\
    & = - \|f'(\wv)\|_2^2 + f(\wv) - f^\star - f'(\wv)^\top (\wv - \wv^\star) \\
    & = - \|f'(\wv)\|_2^2 + f(\wv) + f'(\wv)^\top (\wv^\star - \wv) - f^\star \leq 0
\end{align*}
$$

即$\Ecal$的单调下降

$$
\begin{align*}
    f(\wv) - f^\star \leq \frac{\Ecal(t)}{t} \leq \frac{\Ecal(0)}{t} = \frac{\| \wv_0 - \wv^\star \|_2^2}{2t} = O(1/t)
\end{align*}
$$

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER 应用到机器学习

<span class="blue">序列到类的模式</span>

<br>

输入序列$\xv_1, \ldots, \xv_T$，输出类别标记$\yhat \in [C]$，例如文本分类

<br>

两种模式：

- 序列的最终表示$\av_T$输入给分类器$g$进行分类：$\hat{y} = g(\av_T)$
- 将整个序列的平均状态$\av$输入给分类器$g$进行分类：$\hat{y} = g(\av)$

<img src="../tikz/seq2class.svg" class="width80 center top4">

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 应用到机器学习

<span class="blue">同步的序列到序列模式</span>

</br>

输入序列$\xv_1, \ldots, \xv_T$，同步输出序列$\yhat_1, \ldots, \yhat_T$，例如词性标注

$$
\begin{align*}
    \hat{y}_t = g(\av_t), ~ \forall t \in [T]
\end{align*}
$$

<img src="../tikz/seq2seq-syn.svg" class="width70 center top4">

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 应用到机器学习

<span class="blue">异步的序列到序列模式</span>，也称为<span class="blue">编码器-解码器</span> (encoder-decoder) 模型

</br>

输入序列$\xv_1, \ldots, \xv_T$，输出序列$\yvhat_1, \ldots, \yvhat_S$，不需要同步输出，也不需要保持相同的长度，例如机器翻译、问答系统、图像描述

$$
\begin{align*}
    \av_t & = h_1 (\av_{t-1}, \xv_t), ~ \forall t \in [T] \\
    \av_{T+t} & = h_2 (\av_{T+t-1}, \yvhat_{t-1}), ~ \forall t \in [S] \\
    \yvhat_t & = g(\av_{T+t}), ~ \forall t \in [S]
\end{align*}
$$

<img src="../tikz/seq2seq-asyn.svg" class="width70 center top2">

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER 随时间反向传播

对$\zv = \Wv \av + \bv$有

$$
\begin{align*}
    \frac{\partial z_j}{\partial \Wv} = \av \ev_j^\top, \quad \frac{\partial \zv}{\partial \bv} = \Iv, \quad \frac{\partial \zv}{\partial \av} = \Wv
\end{align*}
$$

</br>

同理对$\zv_k = \Uv \av_{k-1} + \Wv \xv_k + \bv$有

$$
\begin{align*}
    \frac{\partial [\zv_k]_j}{\partial \Uv} = \av_{k-1} \ev_j^\top, \quad \frac{\partial [\zv_k]_j}{\partial \Wv} = \xv_k \ev_j^\top, \quad \frac{\partial \zv_k}{\partial \bv} = \Iv, \quad \frac{\partial \zv_k}{\partial \av_{k-1}} = \Uv
\end{align*}
$$

</br>

随时间反向传播：

- 循环神经网络可以看作一个展开的多层前馈网络，“每层”对应“每个时刻”
- 所有层参数共享，因此参数的真实梯度是所有“展开层”的梯度之和

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 随时间反向传播

对$\zv_k = \Uv \av_{k-1} + \Wv \xv_k + \bv$有

$$
\begin{align*}
    \frac{\partial [\zv_k]_j}{\partial \Uv} = \av_{k-1} \ev_j^\top, \quad \frac{\partial [\zv_k]_j}{\partial \Wv} = \xv_k \ev_j^\top, \quad \frac{\partial \zv_k}{\partial \bv} = \Iv, \quad \frac{\partial \zv_k}{\partial \av_{k-1}} = \Uv
\end{align*}
$$

记时刻$t$的损失为$\Lcal_t$，则总损失为$\Lcal = \sum_{t \in [T]} \Lcal_t$，记$\deltav_{t,k}^\top = \partial \Lcal_t / \partial \zv_k$为时刻$t$的损失对时刻$k$隐藏层输入的导数

注意$\av_k = h(\zv_k)$，由链式法则

$$
\begin{align*}
    \deltav_{t,k}^\top = \frac{\partial \Lcal_t}{\partial \zv_k} = \frac{\partial \Lcal_t}{\partial \zv_{k+1}} \frac{\partial \zv_{k+1}}{\partial \av_k} \frac{\partial \av_k}{\partial \zv_k} = \deltav_{t,k+1}^\top \Uv ~  \diag (h'(\zv_k))
\end{align*}
$$

依然有反向传播的结构

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 随时间反向传播

对$\zv_k = \Uv \av_{k-1} + \Wv \xv_k + \bv$有

$$
\begin{align*}
    \frac{\partial [\zv_k]_j}{\partial \Uv} = \av_{k-1} \ev_j^\top, \quad \frac{\partial [\zv_k]_j}{\partial \Wv} = \xv_k \ev_j^\top, \quad \frac{\partial \zv_k}{\partial \bv} = \Iv, \quad \frac{\partial \zv_k}{\partial \av_{k-1}} = \Uv
\end{align*}
$$

记时刻$t$的损失为$\Lcal_t$，则总损失为$\Lcal = \sum_{t \in [T]} \Lcal_t$，记$\deltav_{t,k}^\top = \partial \Lcal_t / \partial \zv_k$为时刻$t$的损失对时刻$k$隐藏层输入的导数

$$
\begin{align*}
    \frac{\partial \Lcal}{\partial \Uv} & = \sum_{t \in [T]} \sum_{k \in [t]} \sum_j \frac{\partial \Lcal_t}{\partial [\zv_k]_j} \frac{\partial [\zv_k]_j}{\partial \Uv} = \sum_{t \in [T]} \sum_{k \in [t]} \av_{k-1} \deltav_{t,k}^\top \\
    \frac{\partial \Lcal}{\partial \Wv} & = \sum_{t \in [T]} \sum_{k \in [t]} \sum_j \frac{\partial \Lcal_t}{\partial [\zv_k]_j} \frac{\partial [\zv_k]_j}{\partial \Wv} = \sum_{t \in [T]} \sum_{k \in [t]} \xv_k \deltav_{t,k}^\top \\
    \frac{\partial \Lcal}{\partial \bv} & = \sum_{t \in [T]} \sum_{k \in [t]} \frac{\partial \Lcal_t}{\partial \zv_k} \frac{\partial \zv_k}{\partial \bv} = \deltav_{t,k}^\top
\end{align*}
$$

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER 长程依赖问题

设$t > k$，反向传播公式经递推有

$$
\begin{align*}
    \deltav_{t,k}^\top = \deltav_{t,k+1}^\top \Uv ~  \diag (h'(\zv_k))  = \cdots = \deltav_{t,t} ~ \Pi_{\tau=k}^{t-1} \left( \Uv ~ \diag (h'(\zv_k)) \right)
\end{align*}
$$

定义$\gamma = \| \diag (h'(\zv_k)) \Uv^\top \|$

- 若$\gamma > 1$，当$t - k \rightarrow \infty$时，出现梯度爆炸
- 若$\gamma < 1$，当$t - k \rightarrow \infty$时，出现梯度消失

</br>

长程依赖问题：循环神经网络理论上可以建立长时间间隔状态间的依赖关系，但由于梯度爆炸/消失问题，实际上只能学习短期的依赖关系

- 精心挑选激活函数，尽量使得$\| \diag (h'(\zv_k)) \Uv^\top \| \approx 1$，需要足够的炼丹经验
- 梯度爆炸：权重衰减，梯度截断
- 梯度消失：引入残差结构$\av_t = \av_{t-1} + f(\xv_t, \av_{t-1})$，但随着时间$t$增长，$\av_t$会变得越来越大，从而导致隐状态变得饱和，但其存储信息的能力是有限的

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER 门控机制

有选择地加入新的信息，并有选择地遗忘之前累积的信息

- 长短期记忆 (<u>L</u>ong <u>S</u>hort-<u>T</u>erm <u>M</u>emory, LSTM) 网络
- 门控循环单元 (<u>G</u>ated <u>R</u>ecurrent <u>U</u>nit, GRU) 网络

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide data-notes="" -->

GNN-HEADER LSTM 网络

引入一个新的内部状态$\cv_t$专门进行线性的循环信息传递，同时输出信息给隐藏层的外部状态$\av_t$

$$
\begin{align*}
    \cv_t & = \fv_t \odot \cv_{t-1} + \iv_t \odot \widetilde{\cv}_t \\
    \av_t & = \ov_t \odot \tanh(\cv_t)
\end{align*}
$$

其中$\odot$为向量元素乘积

- $\widetilde{\cv}_t = \tanh(\Wv_c \xv_t + \Uv_c \av_{t−1} + \bv_c)$是通过非线性函数得到的候选状态
- 遗忘门$\fv_t = \sigma(\Wv_f \xv_t + \Uv_f \av_{t−1} + \bv_f) \in (0,1)$控制上一个时刻的内部状态$\cv_{t-1}$需要遗忘多少信息
- 输入门$\iv_t = \sigma(\Wv_i \xv_t + \Uv_i \av_{t−1} + \bv_i) \in (0,1)$控制当前时刻的候选状态$\widetilde{\cv}_t$需要保存多少信息
- 输出门$\ov_t = \sigma(\Wv_o \xv_t + \Uv_o \av_{t−1} + \bv_o) \in (0,1)$控制当前时刻的内部状态$\cv_t$需要输出多少信息给外部状态$\av_t$

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn

<!-- slide vertical=true data-notes="" -->

GNN-HEADER LSTM 网络

<img src="../tikz/lstm.svg" class="width80 center top5">

GNN-FOOTER 图神经网络导论 卷积神经网络 tengzhang@hust.edu.cn